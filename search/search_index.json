{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"ex1/","text":"Excersise 1: Server creation Step-by-step guide to creating your first server Step 1: Create a Server on Hetzner Sign up or log in to Hetzner. Under the dropdown menu, go to Cloud and select Projects. Choose the Default project. In the sidebar, click on Servers. Next, click Add Server. Under Create Server, choose the location (please select Nuremberg or Falkenstein). Under Images, choose Debian 12. For the type, select Shared vCPU x86 (Intel/AMD) CX22. SSH Keys are optional, but we used them. Create a unique name (for example: myFirstServer). Click on Pay Now to confirm and create the server. Step 2: Set Root Password in Rescue Mode Click on your newly created server. At the top, go to the Rescue tab. Click Reset Root Password and save the generated password. Example password (don\u2019t use this one in real setups): EnuA7rq3ks7MeuNLPn3H Step 3: Access the Server Console Click the Open Console icon on the right (>_). Log in as root with the new password. If you forget the password, you can find it again under the Rescue tab. Note: Copy-paste doesn\u2019t work in the console, so you\u2019ll need to type it manually. Step 4: (Optional) Fix Keyboard Configuration If you need German keyboard settings: Run: dpkg-reconfigure keyboard-configuration service keyboard-setup restart Make the following selections: Keyboard Model: Generic 105-key PC Keyboard Layout: Other \u2192 German Country of Origin: German AltGr Key: Default (Right Alt) Compose Key: Default (No compose key) 3. Restart the service: service keyboard-setup restart Note: You may need to reboot the server afterward. Step 5: Adjust Firewall to Allow ICMP (Ping) To check if your server is online, we\u2019ll enable ICMP in the firewall: Log in to the Hetzner Cloud Dashboard. In the sidebar, click Firewalls and create a new one. Configure with the following rules: Any IPv4 / Any IPv6 Protocol: TCP \u2014 Port 22 Protocol: ICMP Save and make sure the firewall is linked to your server: Under Assigned Resources , check if your server is listed. If not: click Resources \u2192 Apply to \u2192 Server and select your server. Warning: Sometimes it doesn\u2019t display correctly \u2014 simply refresh or restart Hetzner. Step 6: Delete the Server (if needed) n the Dashboard, go to Servers . Select your server. Click on More (\u2022\u2022\u2022) in the top right corner, or directly click Delete . Confirm the action.","title":"Exercise 1"},{"location":"ex1/#excersise-1-server-creation","text":"Step-by-step guide to creating your first server","title":"Excersise 1: Server creation"},{"location":"ex1/#step-1-create-a-server-on-hetzner","text":"Sign up or log in to Hetzner. Under the dropdown menu, go to Cloud and select Projects. Choose the Default project. In the sidebar, click on Servers. Next, click Add Server. Under Create Server, choose the location (please select Nuremberg or Falkenstein). Under Images, choose Debian 12. For the type, select Shared vCPU x86 (Intel/AMD) CX22. SSH Keys are optional, but we used them. Create a unique name (for example: myFirstServer). Click on Pay Now to confirm and create the server.","title":"Step 1: Create a Server on Hetzner"},{"location":"ex1/#step-2-set-root-password-in-rescue-mode","text":"Click on your newly created server. At the top, go to the Rescue tab. Click Reset Root Password and save the generated password. Example password (don\u2019t use this one in real setups): EnuA7rq3ks7MeuNLPn3H","title":"Step 2: Set Root Password in Rescue Mode"},{"location":"ex1/#step-3-access-the-server-console","text":"Click the Open Console icon on the right (>_). Log in as root with the new password. If you forget the password, you can find it again under the Rescue tab. Note: Copy-paste doesn\u2019t work in the console, so you\u2019ll need to type it manually.","title":"Step 3: Access the Server Console"},{"location":"ex1/#step-4-optional-fix-keyboard-configuration","text":"If you need German keyboard settings: Run: dpkg-reconfigure keyboard-configuration service keyboard-setup restart Make the following selections: Keyboard Model: Generic 105-key PC Keyboard Layout: Other \u2192 German Country of Origin: German AltGr Key: Default (Right Alt) Compose Key: Default (No compose key)","title":"Step 4: (Optional) Fix Keyboard Configuration"},{"location":"ex1/#3-restart-the-service","text":"service keyboard-setup restart Note: You may need to reboot the server afterward.","title":"3. Restart the service:"},{"location":"ex1/#step-5-adjust-firewall-to-allow-icmp-ping","text":"To check if your server is online, we\u2019ll enable ICMP in the firewall: Log in to the Hetzner Cloud Dashboard. In the sidebar, click Firewalls and create a new one. Configure with the following rules: Any IPv4 / Any IPv6 Protocol: TCP \u2014 Port 22 Protocol: ICMP Save and make sure the firewall is linked to your server: Under Assigned Resources , check if your server is listed. If not: click Resources \u2192 Apply to \u2192 Server and select your server. Warning: Sometimes it doesn\u2019t display correctly \u2014 simply refresh or restart Hetzner.","title":"Step 5: Adjust Firewall to Allow ICMP (Ping)"},{"location":"ex1/#step-6-delete-the-server-if-needed","text":"n the Dashboard, go to Servers . Select your server. Click on More (\u2022\u2022\u2022) in the top right corner, or directly click Delete . Confirm the action.","title":"Step 6: Delete the Server (if needed)"},{"location":"ex10/","text":"Excersise 10: Using the tail -f Command Step 1: Connect to the VM Step 2: Check the Authentication Log At first, I tried: tail -f /var/log/auth.log However, this file did not exist. By running ls /var/log/ I found out that the system uses journal instead. So, to view the log in real time, I used: journalctl -u ssh -f There you can also see how many login attempts are being made \u2014 most of them failing. Step 3: Test Successful Login In another terminal, I logged in again via SSH and then checked the logs. For a successful login, the log shows entries like: Accepted publickey for root from <my IP> port <my port> ssh2: RSA <public key fingerprint> session opened for user root(uid=0) by (uid=0)","title":"Exercise 10"},{"location":"ex10/#excersise-10-using-the-tail-f-command","text":"","title":"Excersise 10: Using the tail -f Command"},{"location":"ex10/#step-1-connect-to-the-vm","text":"","title":"Step 1: Connect to the VM"},{"location":"ex10/#step-2-check-the-authentication-log","text":"At first, I tried: tail -f /var/log/auth.log However, this file did not exist. By running ls /var/log/ I found out that the system uses journal instead. So, to view the log in real time, I used: journalctl -u ssh -f There you can also see how many login attempts are being made \u2014 most of them failing.","title":"Step 2: Check the Authentication Log"},{"location":"ex10/#step-3-test-successful-login","text":"In another terminal, I logged in again via SSH and then checked the logs. For a successful login, the log shows entries like: Accepted publickey for root from <my IP> port <my port> ssh2: RSA <public key fingerprint> session opened for user root(uid=0) by (uid=0)","title":"Step 3: Test Successful Login"},{"location":"ex11/","text":"Excersise 11: Creating a Basic Server with Terraform on Hetzner Cloud 1. Minimal Terraform Configuration We started from a minimal Terraform configuration (based on Figure 1004) and defined a Hetzner Cloud server resource. The provider was initialized using the Hetzner API token. Steps: Created a new Terraform project. Initialized the backend using: terraform init Confirmation: Terraform has been successfully initialized! Defined a hcloud_server resource ( exercise-11 ) with: Debian 12 image cpx11 server type 40 GB disk Firewall configuration SSH key access. 2. Firewall Rule for SSH Access An inbound firewall rule was added to allow SSH (port 22) connections. This was necessary to establish secure remote access to the server. 3. Hetzner API Token We entered the Hetzner Cloud API token to enable Terraform to authenticate with Hetzner. Note: Since version control is required, storing the token directly in the configuration would be unsafe. Instead, we followed the solution from Figure 1018 by placing the token in a separate file (not committed to Git). 4. Server Creation and Initial Login Running: terraform apply Terraform created the server successfully: hcloud_server.exercise_11: Creation complete after 13s Hetzner automatically sent an email with the server\u2019s IP address and root password. This happens because no SSH key was yet associated with the server at the time of its first creation. For security, Hetzner provides initial credentials via email. We verified login via: ssh root@<server-ip> 5. Switching to Public/Private Key Authentication To avoid insecure password-based login, we configured SSH key authentication. Steps: Created an hcloud_ssh_key resource with our public key. Attached the SSH key to the server resource in Terraform. Reapplied Terraform configuration. Verified login with: ssh -i ~/.ssh/id_rsa root@<server-ip> Now login works securely with our private key. 6. Outputs Configuration To automatically display the server IP and datacenter, we added an outputs.tf file with: output \"server_ip\" { value = hcloud_server.exercise_11.ipv4_address } output \"server_datacenter\" { value = hcloud_server.exercise_11.datacenter } On success, the output looked like: Apply complete! Resources: 3 added, 0 changed, 0 destroyed. Outputs: server_datacenter = \"hel1-dc2\" server_ip = \"95.216.223.223\" 7. Version Control with Git The Terraform configuration was placed under version control in a Git repository. To avoid leaking secrets, the Hetzner API token was excluded by using .gitignore and a separate file approach.","title":"Exercise 11"},{"location":"ex11/#excersise-11-creating-a-basic-server-with-terraform-on-hetzner-cloud","text":"","title":"Excersise 11: Creating a Basic Server with Terraform on Hetzner Cloud"},{"location":"ex11/#1-minimal-terraform-configuration","text":"We started from a minimal Terraform configuration (based on Figure 1004) and defined a Hetzner Cloud server resource. The provider was initialized using the Hetzner API token. Steps: Created a new Terraform project. Initialized the backend using: terraform init Confirmation: Terraform has been successfully initialized! Defined a hcloud_server resource ( exercise-11 ) with: Debian 12 image cpx11 server type 40 GB disk Firewall configuration SSH key access.","title":"1. Minimal Terraform Configuration"},{"location":"ex11/#2-firewall-rule-for-ssh-access","text":"An inbound firewall rule was added to allow SSH (port 22) connections. This was necessary to establish secure remote access to the server.","title":"2. Firewall Rule for SSH Access"},{"location":"ex11/#3-hetzner-api-token","text":"We entered the Hetzner Cloud API token to enable Terraform to authenticate with Hetzner. Note: Since version control is required, storing the token directly in the configuration would be unsafe. Instead, we followed the solution from Figure 1018 by placing the token in a separate file (not committed to Git).","title":"3. Hetzner API Token"},{"location":"ex11/#4-server-creation-and-initial-login","text":"Running: terraform apply Terraform created the server successfully: hcloud_server.exercise_11: Creation complete after 13s Hetzner automatically sent an email with the server\u2019s IP address and root password. This happens because no SSH key was yet associated with the server at the time of its first creation. For security, Hetzner provides initial credentials via email. We verified login via: ssh root@<server-ip>","title":"4. Server Creation and Initial Login"},{"location":"ex11/#5-switching-to-publicprivate-key-authentication","text":"To avoid insecure password-based login, we configured SSH key authentication. Steps: Created an hcloud_ssh_key resource with our public key. Attached the SSH key to the server resource in Terraform. Reapplied Terraform configuration. Verified login with: ssh -i ~/.ssh/id_rsa root@<server-ip> Now login works securely with our private key.","title":"5. Switching to Public/Private Key Authentication"},{"location":"ex11/#6-outputs-configuration","text":"To automatically display the server IP and datacenter, we added an outputs.tf file with: output \"server_ip\" { value = hcloud_server.exercise_11.ipv4_address } output \"server_datacenter\" { value = hcloud_server.exercise_11.datacenter } On success, the output looked like: Apply complete! Resources: 3 added, 0 changed, 0 destroyed. Outputs: server_datacenter = \"hel1-dc2\" server_ip = \"95.216.223.223\"","title":"6. Outputs Configuration"},{"location":"ex11/#7-version-control-with-git","text":"The Terraform configuration was placed under version control in a Git repository. To avoid leaking secrets, the Hetzner API token was excluded by using .gitignore and a separate file approach.","title":"7. Version Control with Git"},{"location":"ex12/","text":"Excersise 12: Automatic Nginx Installation Using Terraform and Cloud-init 1. Objective The goal of this exercise was to automatically install and configure the Nginx web server on a newly created Hetzner Cloud instance using a user_data script in Terraform. Additionally, SSH password login and root login were disabled, and a dedicated user was created for secure access. 2. Implementation Steps 2.1 Terraform Configuration with Cloud-init A user_data script was integrated into the Terraform configuration. This script was executed during server initialization and performed the following actions: 1. Installed Nginx Used the package manager ( apt-get ) to install the Nginx web server. 2. Started the Nginx Service Activated the service immediately with: systemctl start nginx 3. Enabled Nginx on Boot Configured Nginx to start automatically after server restarts: systemctl enable nginx 2.2 User and Security Configuration The Cloud-init script included additional security hardening: Disabled SSH Password Login ssh_pwauth: false Disabled Root Login disable_root: true Created a New User ( devops ) - Added to the sudo group. - Configured with SSH key authentication. - Verified login with: hcl ssh devops@<server-ip> Attempted root login now returns: \u201cPlease login as the user NONE\u201d , confirming root access is blocked. 2.3 Verification of Nginx After the server was created and initialized, accessing the server\u2019s public IP in a browser displayed the default Nginx landing page: Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. This confirmed that the automated installation and configuration were successful. 3. Outputs The Terraform outputs.tf file was used to display relevant information: output \"server_ip\" { value = hcloud_server.exercise_11.ipv4_address } output \"server_datacenter\" { value = hcloud_server.exercise_11.datacenter } Result after apply: Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: server_datacenter = \"hel1-dc2\" server_ip = \"95.216.223.223\"","title":"Exercise 12"},{"location":"ex12/#excersise-12-automatic-nginx-installation-using-terraform-and-cloud-init","text":"","title":"Excersise 12: Automatic Nginx Installation Using Terraform and Cloud-init"},{"location":"ex12/#1-objective","text":"The goal of this exercise was to automatically install and configure the Nginx web server on a newly created Hetzner Cloud instance using a user_data script in Terraform. Additionally, SSH password login and root login were disabled, and a dedicated user was created for secure access.","title":"1. Objective"},{"location":"ex12/#2-implementation-steps","text":"2.1 Terraform Configuration with Cloud-init A user_data script was integrated into the Terraform configuration. This script was executed during server initialization and performed the following actions: 1. Installed Nginx Used the package manager ( apt-get ) to install the Nginx web server. 2. Started the Nginx Service Activated the service immediately with: systemctl start nginx 3. Enabled Nginx on Boot Configured Nginx to start automatically after server restarts: systemctl enable nginx","title":"2. Implementation Steps"},{"location":"ex12/#22-user-and-security-configuration","text":"The Cloud-init script included additional security hardening: Disabled SSH Password Login ssh_pwauth: false Disabled Root Login disable_root: true Created a New User ( devops ) - Added to the sudo group. - Configured with SSH key authentication. - Verified login with: hcl ssh devops@<server-ip> Attempted root login now returns: \u201cPlease login as the user NONE\u201d , confirming root access is blocked.","title":"2.2 User and Security Configuration"},{"location":"ex12/#23-verification-of-nginx","text":"After the server was created and initialized, accessing the server\u2019s public IP in a browser displayed the default Nginx landing page: Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. This confirmed that the automated installation and configuration were successful.","title":"2.3 Verification of Nginx"},{"location":"ex12/#3-outputs","text":"The Terraform outputs.tf file was used to display relevant information: output \"server_ip\" { value = hcloud_server.exercise_11.ipv4_address } output \"server_datacenter\" { value = hcloud_server.exercise_11.datacenter } Result after apply: Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: server_datacenter = \"hel1-dc2\" server_ip = \"95.216.223.223\"","title":"3. Outputs"},{"location":"ex13/","text":"Excersise 13: Working on Cloud-init with Terraform 1. Objective The goal of this exercise was to extend our base system deployment by using Cloud-init with Terraform to automatically configure a secure web server. Key requirements were: Deploy an Nginx web server with a custom landing page. Enable inbound HTTP traffic (port 80) via firewall configuration. Harden SSH access (no password login, no root login). Create a devops user with SSH key and sudo privileges. Upgrade the Debian 12 distribution during server creation. Install and configure fail2ban for protection against repeated failed SSH login attempts. Install and initialize the plocate file indexer. 2. Terraform and Cloud-init Configuration 2.1 Firewall Setup The firewall configuration allowed inbound traffic for: SSH (TCP port 22) for secure administration. HTTP (TCP port 80) for Nginx. Example snippet: resource \"hcloud_firewall\" \"exercise_13_firewall\" { name = \"exercise-13-firewall\" rule { direction = \"in\" protocol = \"tcp\" port = \"22\" source_ips = [\"0.0.0.0/0\", \"::/0\"] description = \"SSH inbound\" } rule { direction = \"in\" protocol = \"tcp\" port = \"80\" source_ips = [\"0.0.0.0/0\", \"::/0\"] description = \"HTTP inbound\" } } 2.2 Cloud-init User Data Script A Cloud-init script was included in the Terraform configuration. It performed the following tasks automatically at server creation: Installed and started Nginx, and enabled it to start on reboot. Updated and upgraded all Debian packages. Disabled SSH password authentication. Disabled root login. Created user devops with sudo privileges and SSH key access. Installed and configured fail2ban. Installed and initialized plocate for fast file searching. 3. Security and Access Configuration Password login disabled Configured in /etc/ssh/sshd_config : PasswordAuthentication no PermitRootLogin no Root login disabled Attempting ssh root@<ip> now returns: Please login as the user \"NONE\" rather than the user \"root\". Created devops user Login works via: ssh devops@<server-ip> Sudo access confirmed The devops user can switch to root: sudo su - 4. Verification of the Web Server After deployment, visiting the server\u2019s IP in a browser displayed the custom Nginx landing page: I'm Nginx @ \"37.27.219.19\" created Wed Jul 30 11:14:38 PM UTC 2025 This confirmed that: Nginx was installed correctly. The firewall allowed inbound traffic on port 80. The landing page was deployed via Cloud-init. 5. System Update at Creation The Cloud-init script updated and upgraded the Debian 12 system packages automatically during initialization. Verification with: sudo apt update Output confirmed: All packages are up to date. 6. Fail2ban Configuration Fail2ban was installed and configured to monitor SSH login attempts. Verification command: sudo fail2ban-client status sshd```` Expected output includes information about failed login attempts and banned IP addresses: ```hcl |- Currently banned: 2 |- Total banned: 2 `- Banned IP list: 170.64.133.30 213.136.94.219 7. Plocate Installation The plocate package was installed and initialized to enable fast file searches. Tested with: locate ssh_host Output listed SSH host key files: /etc/ssh/ssh_host_ed25519_key /etc/ssh/ssh_host_ed25519_key.pub","title":"Exercise 13"},{"location":"ex13/#excersise-13-working-on-cloud-init-with-terraform","text":"","title":"Excersise 13: Working on Cloud-init with Terraform"},{"location":"ex13/#1-objective","text":"The goal of this exercise was to extend our base system deployment by using Cloud-init with Terraform to automatically configure a secure web server. Key requirements were: Deploy an Nginx web server with a custom landing page. Enable inbound HTTP traffic (port 80) via firewall configuration. Harden SSH access (no password login, no root login). Create a devops user with SSH key and sudo privileges. Upgrade the Debian 12 distribution during server creation. Install and configure fail2ban for protection against repeated failed SSH login attempts. Install and initialize the plocate file indexer.","title":"1. Objective"},{"location":"ex13/#2-terraform-and-cloud-init-configuration","text":"2.1 Firewall Setup The firewall configuration allowed inbound traffic for: SSH (TCP port 22) for secure administration. HTTP (TCP port 80) for Nginx. Example snippet: resource \"hcloud_firewall\" \"exercise_13_firewall\" { name = \"exercise-13-firewall\" rule { direction = \"in\" protocol = \"tcp\" port = \"22\" source_ips = [\"0.0.0.0/0\", \"::/0\"] description = \"SSH inbound\" } rule { direction = \"in\" protocol = \"tcp\" port = \"80\" source_ips = [\"0.0.0.0/0\", \"::/0\"] description = \"HTTP inbound\" } } 2.2 Cloud-init User Data Script A Cloud-init script was included in the Terraform configuration. It performed the following tasks automatically at server creation: Installed and started Nginx, and enabled it to start on reboot. Updated and upgraded all Debian packages. Disabled SSH password authentication. Disabled root login. Created user devops with sudo privileges and SSH key access. Installed and configured fail2ban. Installed and initialized plocate for fast file searching.","title":"2. Terraform and Cloud-init Configuration"},{"location":"ex13/#3-security-and-access-configuration","text":"Password login disabled Configured in /etc/ssh/sshd_config : PasswordAuthentication no PermitRootLogin no Root login disabled Attempting ssh root@<ip> now returns: Please login as the user \"NONE\" rather than the user \"root\". Created devops user Login works via: ssh devops@<server-ip> Sudo access confirmed The devops user can switch to root: sudo su -","title":"3. Security and Access Configuration"},{"location":"ex13/#4-verification-of-the-web-server","text":"After deployment, visiting the server\u2019s IP in a browser displayed the custom Nginx landing page: I'm Nginx @ \"37.27.219.19\" created Wed Jul 30 11:14:38 PM UTC 2025 This confirmed that: Nginx was installed correctly. The firewall allowed inbound traffic on port 80. The landing page was deployed via Cloud-init.","title":"4. Verification of the Web Server"},{"location":"ex13/#5-system-update-at-creation","text":"The Cloud-init script updated and upgraded the Debian 12 system packages automatically during initialization. Verification with: sudo apt update Output confirmed: All packages are up to date.","title":"5. System Update at Creation"},{"location":"ex13/#6-fail2ban-configuration","text":"Fail2ban was installed and configured to monitor SSH login attempts. Verification command: sudo fail2ban-client status sshd```` Expected output includes information about failed login attempts and banned IP addresses: ```hcl |- Currently banned: 2 |- Total banned: 2 `- Banned IP list: 170.64.133.30 213.136.94.219","title":"6. Fail2ban Configuration"},{"location":"ex13/#7-plocate-installation","text":"The plocate package was installed and initialized to enable fast file searches. Tested with: locate ssh_host Output listed SSH host key files: /etc/ssh/ssh_host_ed25519_key /etc/ssh/ssh_host_ed25519_key.pub","title":"7. Plocate Installation"},{"location":"ex14/","text":"Exercise 14: Solving the ~/.ssh/known_hosts Quirk 1. Objective The aim of this exercise was to resolve the recurring problem of SSH host key verification warnings (or failures) when provisioning new servers via Terraform. The solution required automatically generating a known_hosts file and wrapper scripts for ssh and scp to ensure smooth, secure access without manual confirmations. 2. Project Setup A new project folder Excercice_14 was created with the following structure: Excercice_14/ tpl/ # Template scripts for ssh/scp bin/ # Generated wrapper scripts gen/ # Generated SSH keys and knwn_hosts main.tf # Terraform configuration outputs.tf # Terraform outputs cloud_init.yml 3. Templates for Wrapper Scripts 3.1 SSH Wrapper Template (tpl/ssh.sh) #!/usr/bin/env bash GEN_DIR=$(dirname \"$0\")/../gen ssh -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" ${devopsUsername}@${ip} \"$@\" ${devopsUsername} and ${ip} are replaced by Terraform using templatefile(). The \"$@\" allows optional additional SSH arguments. 3.2 SCP Wrapper Template ( tpl/scp.sh ) #!/usr/bin/env bash GEN_DIR=$(dirname \"$0\")/../gen if [ $# -lt 2 ]; then echo \"usage: .../bin/scp <source> <destination>\" else scp -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" \"$@\" fi Ensures at least two arguments are passed (source and destination). Uses the same custom known_hosts file as the SSH wrapper. 4. Terraform Configuration 4.1 Providers and Keys Added the tls and local providers in addition to hetznercloud/hcloud. Generated an ED25519 key pair via Terraform: resource \"tls_private_key\" \"devops_key\" { algorithm = \"ED25519\" } Stored the keys in the gen/ directory. Registered the public key in Hetzner with: resource \"hcloud_ssh_key\" \"devops_key\" { name = \"exercise14-key\" public_key = tls_private_key.devops_key.public_key_openssh } 4.2 Known Hosts File Terraform generated a gen/known_hosts file containing the server IP and host key fingerprint. This file ensures SSH does not show host key verification warnings. 4.3 Server Creation Created a Debian 12 server with Cloud-init. Attached the devops SSH key. Added firewall rules for SSH (22) and HTTP (80). 5. Cloud-init Configuration The Cloud-init script was reused from Exercise 13 and ensured: Installation and enabling of Nginx. Creation of a devops user with sudo privileges. Disabled password authentication ( ssh_pwauth: false ). Disabled root login ( disable_root: true ). 6. Testing and Verification 6.1 Terraform Apply Execution of: terraform apply Output confirmed: Apply complete! Resources: 9 added, 0 changed, 0 destroyed. Outputs: server_datacenter = \"hel1-dc2\" server_ip = \"95.216.223.223\" 6.2 Using the SSH Wrapper ./bin/ssh Successfully logged into the server as devops. No host key warnings appeared. 6.3 File Transfer with SCP Wrapper echo \"Hello from Pauline\" > localfile.txt ./bin/scp localfile.txt devops@95.216.223.223:/home/devops/ On the server: ls -l /home/devops/ cat /home/devops/localfile.txt Output: Hello from Pauline Confirmed that file transfer works correctly.","title":"Exercise 14"},{"location":"ex14/#exercise-14-solving-the-sshknown_hosts-quirk","text":"","title":"Exercise 14: Solving the ~/.ssh/known_hosts Quirk"},{"location":"ex14/#1-objective","text":"The aim of this exercise was to resolve the recurring problem of SSH host key verification warnings (or failures) when provisioning new servers via Terraform. The solution required automatically generating a known_hosts file and wrapper scripts for ssh and scp to ensure smooth, secure access without manual confirmations.","title":"1. Objective"},{"location":"ex14/#2-project-setup","text":"A new project folder Excercice_14 was created with the following structure: Excercice_14/ tpl/ # Template scripts for ssh/scp bin/ # Generated wrapper scripts gen/ # Generated SSH keys and knwn_hosts main.tf # Terraform configuration outputs.tf # Terraform outputs cloud_init.yml","title":"2. Project Setup"},{"location":"ex14/#3-templates-for-wrapper-scripts","text":"3.1 SSH Wrapper Template (tpl/ssh.sh) #!/usr/bin/env bash GEN_DIR=$(dirname \"$0\")/../gen ssh -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" ${devopsUsername}@${ip} \"$@\" ${devopsUsername} and ${ip} are replaced by Terraform using templatefile(). The \"$@\" allows optional additional SSH arguments.","title":"3. Templates for Wrapper Scripts"},{"location":"ex14/#32-scp-wrapper-template-tplscpsh","text":"#!/usr/bin/env bash GEN_DIR=$(dirname \"$0\")/../gen if [ $# -lt 2 ]; then echo \"usage: .../bin/scp <source> <destination>\" else scp -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" \"$@\" fi Ensures at least two arguments are passed (source and destination). Uses the same custom known_hosts file as the SSH wrapper.","title":"3.2 SCP Wrapper Template (tpl/scp.sh)"},{"location":"ex14/#4-terraform-configuration","text":"4.1 Providers and Keys Added the tls and local providers in addition to hetznercloud/hcloud. Generated an ED25519 key pair via Terraform: resource \"tls_private_key\" \"devops_key\" { algorithm = \"ED25519\" } Stored the keys in the gen/ directory. Registered the public key in Hetzner with: resource \"hcloud_ssh_key\" \"devops_key\" { name = \"exercise14-key\" public_key = tls_private_key.devops_key.public_key_openssh } 4.2 Known Hosts File Terraform generated a gen/known_hosts file containing the server IP and host key fingerprint. This file ensures SSH does not show host key verification warnings. 4.3 Server Creation Created a Debian 12 server with Cloud-init. Attached the devops SSH key. Added firewall rules for SSH (22) and HTTP (80).","title":"4. Terraform Configuration"},{"location":"ex14/#5-cloud-init-configuration","text":"The Cloud-init script was reused from Exercise 13 and ensured: Installation and enabling of Nginx. Creation of a devops user with sudo privileges. Disabled password authentication ( ssh_pwauth: false ). Disabled root login ( disable_root: true ).","title":"5. Cloud-init Configuration"},{"location":"ex14/#6-testing-and-verification","text":"6.1 Terraform Apply Execution of: terraform apply Output confirmed: Apply complete! Resources: 9 added, 0 changed, 0 destroyed. Outputs: server_datacenter = \"hel1-dc2\" server_ip = \"95.216.223.223\" 6.2 Using the SSH Wrapper ./bin/ssh Successfully logged into the server as devops. No host key warnings appeared. 6.3 File Transfer with SCP Wrapper echo \"Hello from Pauline\" > localfile.txt ./bin/scp localfile.txt devops@95.216.223.223:/home/devops/ On the server: ls -l /home/devops/ cat /home/devops/localfile.txt Output: Hello from Pauline Confirmed that file transfer works correctly.","title":"6. Testing and Verification"},{"location":"ex15/","text":"Exercise 15: Partitions and Mounting 1. Objective The goal of this exercise was to extend our Terraform-managed server by attaching an additional volume, then manually partitioning and mounting it inside the Linux system. Finally, the setup had to ensure that the mounts persisted across reboots. 2. Terraform Configuration We extended our Terraform configuration to create a new Hetzner Cloud volume and attach it to the server provisioned in Exercise 14. Volume Resource resource \"hcloud_volume\" \"volume01\" { name = \"volume1\" size = 10 server_id = hcloud_server.exercise_14.id automount = true format = \"xfs\" } Outputs output \"volume_id\" { value = hcloud_volume.volume01.id description = \"The volume's id\" } output \"volume_linux_device\" { value = hcloud_volume.volume01.linux_device description = \"The volume's linux device\" } After applying: Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: server_datacenter = \"hel1-dc2\" server_ip = \"95.216.223.223\" volume_id = \"102995739\" volume_linux_device = \"/dev/disk/by-id/scsi-0HC_Volume_102995739\" 3. Detecting the New Volume Initially, the new volume /dev/sdb was not visible. To detect it, we ran: sudo udevadm trigger Afterwards, listing block devices confirmed its presence: lsblk Output (before partitioning): sdb 8:16 0 10G 0 disk 4. Partitioning the Volume We used fdisk to create two primary partitions of ~5 GB each: sudo fdisk /dev/sdb Steps: n \u2192 create new partition Type: p \u2192 primary Size: +5G Repeated for second partition Saved with w Resulting devices: /dev/sdb1 5G /dev/sdb2 5G 5. Creating File Systems We formatted the new partitions with different file systems: sudo mkfs -t ext4 /dev/sdb1 sudo mkfs -t xfs /dev/sdb2 6. Mounting the Partitions We created mount points: sudo mkdir /disk1 /disk2 Mounted them as follows: sudo mount /dev/sdb1 /disk1 sudo mount UUID=$(sudo blkid -s UUID -o value /dev/sdb2) /disk2 Verification: df -h Showed /disk1 and /disk2 mounted successfully. 7. Persistent Mounts with fstab We edited /etc/fstab to ensure partitions mount automatically after reboot. Entries added: /dev/sdb1 /disk1 ext4 defaults 0 0 UUID=<UUID-of-sdb2> /disk2 xfs defaults 0 0 Then tested with: sudo mount -a No errors occurred, confirming the syntax was valid. 8. Verification After Reboot After rebooting the system, both partitions were automatically mounted. df -h Confirmed that /disk1 (ext4) and /disk2 (xfs) were available. 9. Problems Encountered Volume not visible at first \u2192 Solved with sudo udevadm trigger . fstab UUID mistake \u2192 Accidentally typed placeholder instead of real UUID. Corrected using sudo blkid . fstab syntax caution \u2192 Verified with mount -a before reboot to avoid boot issues.","title":"Exercise 15"},{"location":"ex15/#exercise-15-partitions-and-mounting","text":"","title":"Exercise 15: Partitions and Mounting"},{"location":"ex15/#1-objective","text":"The goal of this exercise was to extend our Terraform-managed server by attaching an additional volume, then manually partitioning and mounting it inside the Linux system. Finally, the setup had to ensure that the mounts persisted across reboots.","title":"1. Objective"},{"location":"ex15/#2-terraform-configuration","text":"We extended our Terraform configuration to create a new Hetzner Cloud volume and attach it to the server provisioned in Exercise 14. Volume Resource resource \"hcloud_volume\" \"volume01\" { name = \"volume1\" size = 10 server_id = hcloud_server.exercise_14.id automount = true format = \"xfs\" } Outputs output \"volume_id\" { value = hcloud_volume.volume01.id description = \"The volume's id\" } output \"volume_linux_device\" { value = hcloud_volume.volume01.linux_device description = \"The volume's linux device\" } After applying: Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: server_datacenter = \"hel1-dc2\" server_ip = \"95.216.223.223\" volume_id = \"102995739\" volume_linux_device = \"/dev/disk/by-id/scsi-0HC_Volume_102995739\"","title":"2. Terraform Configuration"},{"location":"ex15/#3-detecting-the-new-volume","text":"Initially, the new volume /dev/sdb was not visible. To detect it, we ran: sudo udevadm trigger Afterwards, listing block devices confirmed its presence: lsblk Output (before partitioning): sdb 8:16 0 10G 0 disk","title":"3. Detecting the New Volume"},{"location":"ex15/#4-partitioning-the-volume","text":"We used fdisk to create two primary partitions of ~5 GB each: sudo fdisk /dev/sdb Steps: n \u2192 create new partition Type: p \u2192 primary Size: +5G Repeated for second partition Saved with w Resulting devices: /dev/sdb1 5G /dev/sdb2 5G","title":"4. Partitioning the Volume"},{"location":"ex15/#5-creating-file-systems","text":"We formatted the new partitions with different file systems: sudo mkfs -t ext4 /dev/sdb1 sudo mkfs -t xfs /dev/sdb2","title":"5. Creating File Systems"},{"location":"ex15/#6-mounting-the-partitions","text":"We created mount points: sudo mkdir /disk1 /disk2 Mounted them as follows: sudo mount /dev/sdb1 /disk1 sudo mount UUID=$(sudo blkid -s UUID -o value /dev/sdb2) /disk2 Verification: df -h Showed /disk1 and /disk2 mounted successfully.","title":"6. Mounting the Partitions"},{"location":"ex15/#7-persistent-mounts-with-fstab","text":"We edited /etc/fstab to ensure partitions mount automatically after reboot. Entries added: /dev/sdb1 /disk1 ext4 defaults 0 0 UUID=<UUID-of-sdb2> /disk2 xfs defaults 0 0 Then tested with: sudo mount -a No errors occurred, confirming the syntax was valid.","title":"7. Persistent Mounts with fstab"},{"location":"ex15/#8-verification-after-reboot","text":"After rebooting the system, both partitions were automatically mounted. df -h Confirmed that /disk1 (ext4) and /disk2 (xfs) were available.","title":"8. Verification After Reboot"},{"location":"ex15/#9-problems-encountered","text":"Volume not visible at first \u2192 Solved with sudo udevadm trigger . fstab UUID mistake \u2192 Accidentally typed placeholder instead of real UUID. Corrected using sudo blkid . fstab syntax caution \u2192 Verified with mount -a before reboot to avoid boot issues.","title":"9. Problems Encountered"},{"location":"ex16/","text":"Exercise 16: Mount Point\u2019s Name Specification 1. Goal The goal of this exercise was to replace the auto-generated Cloud-init mount point (e.g., /mnt/HC_Volume_102626366 ) with a custom mount point /volume01 . This required creating the server and volume independently, attaching the volume manually ( automount = false ), and configuring the mount via Cloud-init so that it persists after reboot. 2. Terraform Configuration We extended the configuration from Exercise 15. Server and volume created independently Both placed in the same location ( hel1 ) Volume attached manually with automount = false Passed the Linux device path and mount point name to Cloud-init Terraform Snippet resource \"hcloud_volume\" \"volume01\" { name = \"volume1-ex16\" size = 10 location = \"hel1\" format = \"xfs\" } resource \"hcloud_volume_attachment\" \"volAttach\" { volume_id = hcloud_volume.volume01.id server_id = hcloud_server.exercise_14.id automount = false } output \"volume_linux_device\" { description = \"The volume's linux device\" value = hcloud_volume.volume01.linux_device } 3. Cloud-init Configuration We modified the Cloud-init template to: Create the mount point /volume01 Append an entry to /etc/fstab using the - UUID of the volume Reload systemd to account for fstab changes Mount the volume immediately Cloud-init Snippet #cloud-config runcmd: - mkdir -p /volume01 - echo \"UUID=a3de7773-6b5e-4f51-886a-bbccf85d0b32 /volume01 xfs defaults 0 0\" >> /etc/fstab - systemctl daemon-reload - mount -a 4. Deployment and Verification Applied the configuration: terraform apply Verified that the new volume appeared: lsblk Result showed /dev/sdb with 10 GB. Mounted the volume manually for testing: sudo mkdir -p /volume01 sudo mount /dev/disk/by-id/scsi-0HC_Volume_102995935 /volume01 Checked the UUID and updated /etc/fstab: sudo blkid /dev/disk/by-id/scsi-0HC_Volume_102995935 Reloaded systemd and mounted all filesystems: sudo systemctl daemon-reload sudo mount -a Verified with: df -h Output confirmed: /dev/sdb 10G 104M 9.9G 2% /volume01 Tested write access: echo \"Test successful!\" | sudo tee /volume01/test.txt","title":"Exercise 16"},{"location":"ex16/#exercise-16-mount-points-name-specification","text":"","title":"Exercise 16: Mount Point\u2019s Name Specification"},{"location":"ex16/#1-goal","text":"The goal of this exercise was to replace the auto-generated Cloud-init mount point (e.g., /mnt/HC_Volume_102626366 ) with a custom mount point /volume01 . This required creating the server and volume independently, attaching the volume manually ( automount = false ), and configuring the mount via Cloud-init so that it persists after reboot.","title":"1. Goal"},{"location":"ex16/#2-terraform-configuration","text":"We extended the configuration from Exercise 15. Server and volume created independently Both placed in the same location ( hel1 ) Volume attached manually with automount = false Passed the Linux device path and mount point name to Cloud-init","title":"2. Terraform Configuration"},{"location":"ex16/#terraform-snippet","text":"resource \"hcloud_volume\" \"volume01\" { name = \"volume1-ex16\" size = 10 location = \"hel1\" format = \"xfs\" } resource \"hcloud_volume_attachment\" \"volAttach\" { volume_id = hcloud_volume.volume01.id server_id = hcloud_server.exercise_14.id automount = false } output \"volume_linux_device\" { description = \"The volume's linux device\" value = hcloud_volume.volume01.linux_device }","title":"Terraform Snippet"},{"location":"ex16/#3-cloud-init-configuration","text":"We modified the Cloud-init template to: Create the mount point /volume01 Append an entry to /etc/fstab using the - UUID of the volume Reload systemd to account for fstab changes Mount the volume immediately","title":"3. Cloud-init Configuration"},{"location":"ex16/#cloud-init-snippet","text":"#cloud-config runcmd: - mkdir -p /volume01 - echo \"UUID=a3de7773-6b5e-4f51-886a-bbccf85d0b32 /volume01 xfs defaults 0 0\" >> /etc/fstab - systemctl daemon-reload - mount -a","title":"Cloud-init Snippet"},{"location":"ex16/#4-deployment-and-verification","text":"Applied the configuration: terraform apply Verified that the new volume appeared: lsblk Result showed /dev/sdb with 10 GB. Mounted the volume manually for testing: sudo mkdir -p /volume01 sudo mount /dev/disk/by-id/scsi-0HC_Volume_102995935 /volume01 Checked the UUID and updated /etc/fstab: sudo blkid /dev/disk/by-id/scsi-0HC_Volume_102995935 Reloaded systemd and mounted all filesystems: sudo systemctl daemon-reload sudo mount -a Verified with: df -h Output confirmed: /dev/sdb 10G 104M 9.9G 2% /volume01 Tested write access: echo \"Test successful!\" | sudo tee /volume01/test.txt","title":"4. Deployment and Verification"},{"location":"ex17/","text":"Exercise 17: SSH Known Hosts Module Objective The objective of this exercise was to create a reusable Terraform module called SshKnownHosts that automatically generates: a known_hosts file, SSH and SCP wrapper scripts so that connecting to servers can be done securely and consistently without repeating configuration snippets in every project. 2. Project Structure We organized the project into two main directories: . KnownHostsByModule bin ssh scp gen known_hosts main.tf network.tf outputs.tf providers.tf Readme.md tpl userData.yml variables.tf Modules SshKnownHosts main.tf Readme.md tpl ssh.sh scp.sh variables.tf Modules/SshKnownHosts : reusable Terraform module for known_hosts handling KnownHostsByModule : main project using the module 3. Module Definition Variables ( Modules/SshKnownHosts/variables.tf ) variable \"loginUserName\" { description = \"The SSH login user\" type = string } variable \"serverNameOrIp\" { description = \"Server IPv4 address or hostname\" type = string } variable \"serverHostPublicKey\" { description = \"The server's host public key\" type = string } Resources ( Modules/SshKnownHosts/main.tf ) Generate known_hosts resource \"local_file\" \"known_hosts\" { content = \"${var.serverNameOrIp} ${var.serverHostPublicKey}\" filename = \"${path.module}/../KnownHostsByModule/gen/known_hosts\" } Generate ssh wrapper resource \"local_file\" \"ssh_script\" { content = templatefile(\"${path.module}/tpl/ssh.sh\", { loginUser = var.loginUserName server_ip = var.serverNameOrIp }) filename = \"${path.module}/../KnownHostsByModule/bin/ssh\" file_permission = \"700\" } Generate scp wrapper resource \"local_file\" \"scp_script\" { content = templatefile(\"${path.module}/tpl/scp.sh\", { loginUser = var.loginUserName server_ip = var.serverNameOrIp }) filename = \"${path.module}/../KnownHostsByModule/bin/scp\" file_permission = \"700\" } 4. Wrapper Templates tpl/ssh.sh #!/bin/bash GEN_DIR=$(dirname \"$0\")/../gen ssh -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" ${loginUser}@${server_ip} \"$@\" tpl/scp.sh #!/bin/bash GEN_DIR=$(dirname \"$0\")/../gen if [ $# -lt 2 ]; then echo \"Usage: ./bin/scp <source> <destination>\" else scp -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" \"$@\" fi 5. Module Usage in Main Project module \"createSshKnownHosts\" { source = \"../Modules/SshKnownHosts\" loginUserName = hcloud_ssh_key.loginUser.name serverNameOrIp = hcloud_server.exercise_14.ipv4_address serverHostPublicKey = tls_private_key.host.public_key_openssh } 6. Verification Applied Terraform configuration: terraform apply Confirmed generation of: gen/known_hosts Executable scripts in bin/ssh and bin/scp Set permissions: chmod +x bin/ssh bin/scp Tested SSH login: ./bin/ssh \u2192 Logged in successfully without host key warning. Tested SCP file transfer: ./bin/scp localfile.txt devops@<server_ip>:/home/devops/ \u2192 File appeared on server.","title":"Exercise 17"},{"location":"ex17/#exercise-17-ssh-known-hosts-module","text":"Objective The objective of this exercise was to create a reusable Terraform module called SshKnownHosts that automatically generates: a known_hosts file, SSH and SCP wrapper scripts so that connecting to servers can be done securely and consistently without repeating configuration snippets in every project.","title":"Exercise 17: SSH Known Hosts Module"},{"location":"ex17/#2-project-structure","text":"We organized the project into two main directories: . KnownHostsByModule bin ssh scp gen known_hosts main.tf network.tf outputs.tf providers.tf Readme.md tpl userData.yml variables.tf Modules SshKnownHosts main.tf Readme.md tpl ssh.sh scp.sh variables.tf Modules/SshKnownHosts : reusable Terraform module for known_hosts handling KnownHostsByModule : main project using the module","title":"2. Project Structure"},{"location":"ex17/#3-module-definition","text":"Variables ( Modules/SshKnownHosts/variables.tf ) variable \"loginUserName\" { description = \"The SSH login user\" type = string } variable \"serverNameOrIp\" { description = \"Server IPv4 address or hostname\" type = string } variable \"serverHostPublicKey\" { description = \"The server's host public key\" type = string }","title":"3. Module Definition"},{"location":"ex17/#resources-modulessshknownhostsmaintf","text":"Generate known_hosts resource \"local_file\" \"known_hosts\" { content = \"${var.serverNameOrIp} ${var.serverHostPublicKey}\" filename = \"${path.module}/../KnownHostsByModule/gen/known_hosts\" } Generate ssh wrapper resource \"local_file\" \"ssh_script\" { content = templatefile(\"${path.module}/tpl/ssh.sh\", { loginUser = var.loginUserName server_ip = var.serverNameOrIp }) filename = \"${path.module}/../KnownHostsByModule/bin/ssh\" file_permission = \"700\" } Generate scp wrapper resource \"local_file\" \"scp_script\" { content = templatefile(\"${path.module}/tpl/scp.sh\", { loginUser = var.loginUserName server_ip = var.serverNameOrIp }) filename = \"${path.module}/../KnownHostsByModule/bin/scp\" file_permission = \"700\" }","title":"Resources (Modules/SshKnownHosts/main.tf)"},{"location":"ex17/#4-wrapper-templates","text":"tpl/ssh.sh #!/bin/bash GEN_DIR=$(dirname \"$0\")/../gen ssh -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" ${loginUser}@${server_ip} \"$@\" tpl/scp.sh #!/bin/bash GEN_DIR=$(dirname \"$0\")/../gen if [ $# -lt 2 ]; then echo \"Usage: ./bin/scp <source> <destination>\" else scp -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" \"$@\" fi","title":"4. Wrapper Templates"},{"location":"ex17/#5-module-usage-in-main-project","text":"module \"createSshKnownHosts\" { source = \"../Modules/SshKnownHosts\" loginUserName = hcloud_ssh_key.loginUser.name serverNameOrIp = hcloud_server.exercise_14.ipv4_address serverHostPublicKey = tls_private_key.host.public_key_openssh }","title":"5. Module Usage in Main Project"},{"location":"ex17/#6-verification","text":"Applied Terraform configuration: terraform apply Confirmed generation of: gen/known_hosts Executable scripts in bin/ssh and bin/scp Set permissions: chmod +x bin/ssh bin/scp Tested SSH login: ./bin/ssh \u2192 Logged in successfully without host key warning. Tested SCP file transfer: ./bin/scp localfile.txt devops@<server_ip>:/home/devops/ \u2192 File appeared on server.","title":"6. Verification"},{"location":"ex18/","text":"Exercise 18: Enhancing Your Web Server Objective The goal of this exercise was to enhance our existing web server setup by: Adding DNS A records to resolve a human-readable domain to our server\u2019s IP. Configuring Nginx to serve the site under the new domain. Securing the site with HTTPS using Let\u2019s Encrypt TLS certificates. Steps: 1. DNS A Records Setup We created two DNS entries pointing to our server\u2019s IP ( 95.216.223.223 ) using nsupdate with the provided key: nsupdate -k ./g1.key > server ns1.hdm-stuttgart.cloud > update add www.g1.sdi.hdm-stuttgart.cloud 10 A 95.216.223.223 > update add g1.sdi.hdm-stuttgart.cloud 10 A 95.216.223.223 > send > quit Verification with dig confirmed the DNS resolution worked: dig +noall +answer @ns1.hdm-stuttgart.cloud g1.sdi.hdm-stuttgart.cloud Result: g1.sdi.hdm-stuttgart.cloud. 10 IN A 95.216.223.223 Both g1.sdi.hdm-stuttgart.cloud and www.g1.sdi.hdm-stuttgart.cloud resolved to the server IP. 2. Configuring Nginx for the Domain We set up a new Nginx server block to handle requests for both domain names: /etc/nginx/sites-available/g1.sdi.hdm-stuttgart.cloud server { listen 80; listen [::]:80; root /var/www/g1.sdi.hdm-stuttgart.cloud/html; index index.html index.htm; server_name g1.sdi.hdm-stuttgart.cloud www.g1.sdi.hdm-stuttgart.cloud; location / { try_files $uri $uri/ =404; } } Then we tested the configuration and restarted Nginx: sudo nginx -t sudo systemctl restart nginx Opening the domain in the browser showed the Nginx landing page with our custom message. 3. Securing with Let\u2019s Encrypt TLS Certificates We installed and used Certbot to request certificates for both domain variants. First, we used a staging certificate to avoid Let\u2019s Encrypt rate limits: sudo certbot --nginx --test-cert -d g1.sdi.hdm-stuttgart.cloud -d www.g1.sdi.hdm-stuttgart.cloud After verifying success, we requested the real certificate: sudo certbot --nginx -d g1.sdi.hdm-stuttgart.cloud -d www.g1.sdi.hdm-stuttgart.cloud Finally, we confirmed automatic renewal works: sudo certbot renew --dry-run Output confirmed that the certificates would renew successfully. Final Verification Access to both http://g1.sdi.hdm-stuttgart.cloud and http://www.g1.sdi.hdm-stuttgart.cloud worked. HTTPS access confirmed with a valid Let\u2019s Encrypt certificate: https://g1.sdi.hdm-stuttgart.cloud https://www.g1.sdi.hdm-stuttgart.cloud Automatic certificate renewal tested successfully. Screenshot evidence: DNS update and verification with dig . Browser access via domain name. Certbot successful certificate deployment. Dry-run renewal showing success.","title":"Exercise 18"},{"location":"ex18/#exercise-18-enhancing-your-web-server","text":"","title":"Exercise 18: Enhancing Your Web Server"},{"location":"ex18/#objective","text":"The goal of this exercise was to enhance our existing web server setup by: Adding DNS A records to resolve a human-readable domain to our server\u2019s IP. Configuring Nginx to serve the site under the new domain. Securing the site with HTTPS using Let\u2019s Encrypt TLS certificates.","title":"Objective"},{"location":"ex18/#steps","text":"","title":"Steps:"},{"location":"ex18/#1-dns-a-records-setup","text":"We created two DNS entries pointing to our server\u2019s IP ( 95.216.223.223 ) using nsupdate with the provided key: nsupdate -k ./g1.key > server ns1.hdm-stuttgart.cloud > update add www.g1.sdi.hdm-stuttgart.cloud 10 A 95.216.223.223 > update add g1.sdi.hdm-stuttgart.cloud 10 A 95.216.223.223 > send > quit Verification with dig confirmed the DNS resolution worked: dig +noall +answer @ns1.hdm-stuttgart.cloud g1.sdi.hdm-stuttgart.cloud Result: g1.sdi.hdm-stuttgart.cloud. 10 IN A 95.216.223.223 Both g1.sdi.hdm-stuttgart.cloud and www.g1.sdi.hdm-stuttgart.cloud resolved to the server IP.","title":"1. DNS A Records Setup"},{"location":"ex18/#2-configuring-nginx-for-the-domain","text":"We set up a new Nginx server block to handle requests for both domain names: /etc/nginx/sites-available/g1.sdi.hdm-stuttgart.cloud server { listen 80; listen [::]:80; root /var/www/g1.sdi.hdm-stuttgart.cloud/html; index index.html index.htm; server_name g1.sdi.hdm-stuttgart.cloud www.g1.sdi.hdm-stuttgart.cloud; location / { try_files $uri $uri/ =404; } } Then we tested the configuration and restarted Nginx: sudo nginx -t sudo systemctl restart nginx Opening the domain in the browser showed the Nginx landing page with our custom message.","title":"2. Configuring Nginx for the Domain"},{"location":"ex18/#3-securing-with-lets-encrypt-tls-certificates","text":"We installed and used Certbot to request certificates for both domain variants. First, we used a staging certificate to avoid Let\u2019s Encrypt rate limits: sudo certbot --nginx --test-cert -d g1.sdi.hdm-stuttgart.cloud -d www.g1.sdi.hdm-stuttgart.cloud After verifying success, we requested the real certificate: sudo certbot --nginx -d g1.sdi.hdm-stuttgart.cloud -d www.g1.sdi.hdm-stuttgart.cloud Finally, we confirmed automatic renewal works: sudo certbot renew --dry-run Output confirmed that the certificates would renew successfully.","title":"3. Securing with Let\u2019s Encrypt TLS Certificates"},{"location":"ex18/#final-verification","text":"Access to both http://g1.sdi.hdm-stuttgart.cloud and http://www.g1.sdi.hdm-stuttgart.cloud worked. HTTPS access confirmed with a valid Let\u2019s Encrypt certificate: https://g1.sdi.hdm-stuttgart.cloud https://www.g1.sdi.hdm-stuttgart.cloud Automatic certificate renewal tested successfully.","title":"Final Verification"},{"location":"ex18/#screenshot-evidence","text":"DNS update and verification with dig . Browser access via domain name. Certbot successful certificate deployment. Dry-run renewal showing success.","title":"Screenshot evidence:"},{"location":"ex19/","text":"Exercise 19: Creating DNS Records with Terraform Implementation 1. Variables Defined in variables.tf and config.auto.tfvars : serverIp = \"1.2.3.4\" dnsZone = \"g1.sdi.hdm-stuttgart.cloud\" serverName = \"workhorse\" serverAliases = [\"www\", \"mail\"] Validation rules ensured: No duplicate aliases ( distinct function). No alias equal to the canonical name ( contains function). 2. Providers Hetzner Cloud Provider \u2192 for server and firewall. DNS Provider (hashicorp/dns) \u2192 with HMAC-SHA512 authentication against ns1.hdm-stuttgart.cloud . 3. Terraform Resources A Record for Base Domain: resource \"dns_a_record_set\" \"base_record\" { zone = \"${var.dnsZone}.\" addresses = [var.serverIp] ttl = 10 } A Record for Canonical Server Name: resource \"dns_a_record_set\" \"server_record\" { zone = \"${var.dnsZone}.\" name = var.serverName addresses = [var.serverIp] ttl = 10 } CNAME Records for Aliases: resource \"dns_cname_record\" \"alias_records\" { for_each = toset(var.serverAliases) zone = \"${var.dnsZone}.\" name = each.key cname = \"${var.serverName}.${var.dnsZone}.\" ttl = 10 } Verification Executed: dig +noall +answer \\ @ns1.hdm-stuttgart.cloud \\ -y hmac-sha512:g1.key:<SECRET> \\ -t AXFR g1.sdi.hdm-stuttgart.cloud Output: g1.sdi.hdm-stuttgart.cloud. 10 IN A 157.180.35.105 workhorse.g1.sdi.hdm-stuttgart.cloud. 10 IN A 157.180.35.105 mail.g1.sdi.hdm-stuttgart.cloud. 10 IN CNAME workhorse.g1.sdi.hdm-stuttgart.cloud. www.g1.sdi.hdm-stuttgart.cloud. 10 IN CNAME workhorse.g1.sdi.hdm-stuttgart.cloud.","title":"Exercise 19"},{"location":"ex19/#exercise-19-creating-dns-records-with-terraform","text":"","title":"Exercise 19: Creating DNS Records with Terraform"},{"location":"ex19/#implementation","text":"","title":"Implementation"},{"location":"ex19/#1-variables","text":"Defined in variables.tf and config.auto.tfvars : serverIp = \"1.2.3.4\" dnsZone = \"g1.sdi.hdm-stuttgart.cloud\" serverName = \"workhorse\" serverAliases = [\"www\", \"mail\"] Validation rules ensured: No duplicate aliases ( distinct function). No alias equal to the canonical name ( contains function).","title":"1. Variables"},{"location":"ex19/#2-providers","text":"Hetzner Cloud Provider \u2192 for server and firewall. DNS Provider (hashicorp/dns) \u2192 with HMAC-SHA512 authentication against ns1.hdm-stuttgart.cloud .","title":"2. Providers"},{"location":"ex19/#3-terraform-resources","text":"A Record for Base Domain: resource \"dns_a_record_set\" \"base_record\" { zone = \"${var.dnsZone}.\" addresses = [var.serverIp] ttl = 10 } A Record for Canonical Server Name: resource \"dns_a_record_set\" \"server_record\" { zone = \"${var.dnsZone}.\" name = var.serverName addresses = [var.serverIp] ttl = 10 } CNAME Records for Aliases: resource \"dns_cname_record\" \"alias_records\" { for_each = toset(var.serverAliases) zone = \"${var.dnsZone}.\" name = each.key cname = \"${var.serverName}.${var.dnsZone}.\" ttl = 10 }","title":"3. Terraform Resources"},{"location":"ex19/#verification","text":"Executed: dig +noall +answer \\ @ns1.hdm-stuttgart.cloud \\ -y hmac-sha512:g1.key:<SECRET> \\ -t AXFR g1.sdi.hdm-stuttgart.cloud Output: g1.sdi.hdm-stuttgart.cloud. 10 IN A 157.180.35.105 workhorse.g1.sdi.hdm-stuttgart.cloud. 10 IN A 157.180.35.105 mail.g1.sdi.hdm-stuttgart.cloud. 10 IN CNAME workhorse.g1.sdi.hdm-stuttgart.cloud. www.g1.sdi.hdm-stuttgart.cloud. 10 IN CNAME workhorse.g1.sdi.hdm-stuttgart.cloud.","title":"Verification"},{"location":"ex2/","text":"Excersise 2: Server Re-Creation Why does this happen? When you first connected to your old server, SSH saved the host key (a kind of fingerprint) in the ~/.ssh/known_hosts file. After creating a completely new server, it now has a different host key. Since the IP address is the same but the key changed, SSH shows a warning: \u201cWARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!\u201d This is a security feature that protects you from a man-in-the-middle attack. How to solve this issue You need to remove the old key associated with the IP from your local known_hosts file. Run the following command (replace the IP with your server\u2019s): ssh-keygen -R 37.27.219.19 Try connecting to the server again via SSH: ssh root@37.27.219.19 You\u2019ll be asked to confirm the new key: Type yes and press Enter. The new key will automatically be saved in your known_hosts .","title":"Exercise 2"},{"location":"ex2/#excersise-2-server-re-creation","text":"","title":"Excersise 2: Server Re-Creation"},{"location":"ex2/#why-does-this-happen","text":"When you first connected to your old server, SSH saved the host key (a kind of fingerprint) in the ~/.ssh/known_hosts file. After creating a completely new server, it now has a different host key. Since the IP address is the same but the key changed, SSH shows a warning: \u201cWARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!\u201d This is a security feature that protects you from a man-in-the-middle attack.","title":"Why does this happen?"},{"location":"ex2/#how-to-solve-this-issue","text":"You need to remove the old key associated with the IP from your local known_hosts file. Run the following command (replace the IP with your server\u2019s): ssh-keygen -R 37.27.219.19 Try connecting to the server again via SSH: ssh root@37.27.219.19 You\u2019ll be asked to confirm the new key: Type yes and press Enter. The new key will automatically be saved in your known_hosts .","title":"How to solve this issue"},{"location":"ex20/","text":"Exercise 20: Creating a Host with Corresponding DNS Entries Implementation 1. Extending Terraform with the SSH Known Hosts Module In main.tf , the createSshKnownHosts module was used with the DNS name instead of the raw IP: module \"createSshKnownHosts\" { source = \"../Modules/SshKnownHosts\" loginUserName = \"devops\" serverNameOrIp = \"${var.serverName}.${var.dnsZone}\" serverHostPublicKey = tls_private_key.host.public_key_openssh } serverNameOrIp now points to workhorse.g1.sdi.hdm-stuttgart.cloud . This ensures generated SSH and SCP wrapper scripts, as well as the known_hosts file, rely on the DNS hostname. 2. Planning and Applying Terraform Ran the following commands: terraform plan -var-file=\"secrets.tfvars\" terraform apply -var-file=\"secrets.tfvars\" Result: Plan: 3 to add, 0 to change, 0 to destroy. Apply complete! Resources: 3 added, 0 changed, 0 destroyed. Generated resources: gen/known_hosts bin/ssh bin/scp 3. Generated Files gen/known_hosts workhorse.g1.sdi.hdm-stuttgart.cloud ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBGurzVlZ0fp9cI6WYpuyFsrFTlpWWLJp4EU3iXj4Voy bin/ssh #!/bin/bash ssh -o UserKnownHostsFile=$(dirname \"$0\")/../gen/known_hosts \\ devops@workhorse.g1.sdi.hdm-stuttgart.cloud \"$@\" bin/scp #!/bin/bash scp -o UserKnownHostsFile=$(dirname \"$0\")/../gen/known_hosts \\ \"$@\" devops@workhorse.g1.sdi.hdm-stuttgart.cloud: 4. Verification Executed: ./bin/ssh Result: Successful login to the Debian host via DNS name. No more IP mismatch warnings. SSH and SCP commands now resolve cleanly using the DNS hostname.","title":"Exercise 20"},{"location":"ex20/#exercise-20-creating-a-host-with-corresponding-dns-entries","text":"","title":"Exercise 20: Creating a Host with Corresponding DNS Entries"},{"location":"ex20/#implementation","text":"","title":"Implementation"},{"location":"ex20/#1-extending-terraform-with-the-ssh-known-hosts-module","text":"In main.tf , the createSshKnownHosts module was used with the DNS name instead of the raw IP: module \"createSshKnownHosts\" { source = \"../Modules/SshKnownHosts\" loginUserName = \"devops\" serverNameOrIp = \"${var.serverName}.${var.dnsZone}\" serverHostPublicKey = tls_private_key.host.public_key_openssh } serverNameOrIp now points to workhorse.g1.sdi.hdm-stuttgart.cloud . This ensures generated SSH and SCP wrapper scripts, as well as the known_hosts file, rely on the DNS hostname.","title":"1. Extending Terraform with the SSH Known Hosts Module"},{"location":"ex20/#2-planning-and-applying-terraform","text":"Ran the following commands: terraform plan -var-file=\"secrets.tfvars\" terraform apply -var-file=\"secrets.tfvars\" Result: Plan: 3 to add, 0 to change, 0 to destroy. Apply complete! Resources: 3 added, 0 changed, 0 destroyed. Generated resources: gen/known_hosts bin/ssh bin/scp","title":"2. Planning and Applying Terraform"},{"location":"ex20/#3-generated-files","text":"gen/known_hosts workhorse.g1.sdi.hdm-stuttgart.cloud ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBGurzVlZ0fp9cI6WYpuyFsrFTlpWWLJp4EU3iXj4Voy bin/ssh #!/bin/bash ssh -o UserKnownHostsFile=$(dirname \"$0\")/../gen/known_hosts \\ devops@workhorse.g1.sdi.hdm-stuttgart.cloud \"$@\" bin/scp #!/bin/bash scp -o UserKnownHostsFile=$(dirname \"$0\")/../gen/known_hosts \\ \"$@\" devops@workhorse.g1.sdi.hdm-stuttgart.cloud:","title":"3. Generated Files"},{"location":"ex20/#4-verification","text":"Executed: ./bin/ssh Result: Successful login to the Debian host via DNS name. No more IP mismatch warnings. SSH and SCP commands now resolve cleanly using the DNS hostname.","title":"4. Verification"},{"location":"ex3/","text":"Excercise 3: Improve your server's security! Step 1: Create a Firewall If an old server is still running, delete it first: - Go to Hetzner Cloud Dashboard \u2192 Select Server \u2192 Click Delete . Create a new firewall: - In the sidebar, click Firewalls. - At the bottom right, click the + (Add) button. Under Rules (Inbound), configure: Allow SSH access: Direction: In Protocol: TCP Port: 22 Allow Ping (ICMP): Direction: In Protocol: ICMP Save the new firewall and give it a name, e.g., secure-firewall . Step 2: Add an SSH Key Go to Security \u2192 SSH Keys \u2192 Add SSH Key . Paste your public key (not the private key!). On Mac, you can display your public key with: cat ~/.ssh/id_ed25519.pub 3. Set the new SSH key as the default. Create a new server (same process as before). Location: Falkenstein or Nuremberg Image: Debian 12 Type: CX22 (Shared vCPU) Firewall: Select your created secure-firewall SSH Key: Select the public key you just added Name: For example, secure-server Click Create & Pay Now. Step 4: Test the Server in the Terminal Ping the server using its IPv4 address: ping 162.55.178.186 Find the IP under Servers \u2192 Select your server (secure-server). Connect via SSH: ssh root@162.55.178.186 Fix Known Hosts Error If you get a host key mismatch error: List your hidden .ssh directory: ls -al ~/.ssh 2.Open the known_hosts file: nano ~/.ssh/known_hosts Remove all entries for this server (usually the last three). Tip for Mac: Install micro for easier file editing: brew install micro Retry the SSH login after saving the file. Example root password (do not use in real setup): xTixqiReWkaLif3qhnN3 Step 5: Update & Reboot Update all packages: apt update && apt upgrade -y Reboot the server: reboot Log in again via SSH. Step 6: Install & Test Nginx Install Nginx: apt install nginx -y Check status: systemctl status nginx If you see active (running) \u2192 Nginx is working. Test locally: wget -O - http://162.55.178.186 If you see HTML with \u201cWelcome to nginx!\u201d, it\u2019s running. Step 7: Enable Browser Access Open in your browser: http://162.55.178.186 If it doesn\u2019t load, the firewall is blocking port 80. Fix the firewall: Go to Firewalls \u2192 secure-firewall \u2192 Edit. Add a new inbound rule: Direction: In Protocol: TCP Port: 80 Save the changes. Test again in your browser. You should now see Welcome to nginx!.","title":"Exercise 3"},{"location":"ex3/#excercise-3-improve-your-servers-security","text":"","title":"Excercise 3: Improve your server's security!"},{"location":"ex3/#step-1-create-a-firewall","text":"If an old server is still running, delete it first: - Go to Hetzner Cloud Dashboard \u2192 Select Server \u2192 Click Delete . Create a new firewall: - In the sidebar, click Firewalls. - At the bottom right, click the + (Add) button. Under Rules (Inbound), configure: Allow SSH access: Direction: In Protocol: TCP Port: 22 Allow Ping (ICMP): Direction: In Protocol: ICMP Save the new firewall and give it a name, e.g., secure-firewall .","title":"Step 1: Create a Firewall"},{"location":"ex3/#step-2-add-an-ssh-key","text":"Go to Security \u2192 SSH Keys \u2192 Add SSH Key . Paste your public key (not the private key!). On Mac, you can display your public key with: cat ~/.ssh/id_ed25519.pub","title":"Step 2: Add an SSH Key"},{"location":"ex3/#3-set-the-new-ssh-key-as-the-default","text":"Create a new server (same process as before). Location: Falkenstein or Nuremberg Image: Debian 12 Type: CX22 (Shared vCPU) Firewall: Select your created secure-firewall SSH Key: Select the public key you just added Name: For example, secure-server Click Create & Pay Now.","title":"3. Set the new SSH key as the default."},{"location":"ex3/#step-4-test-the-server-in-the-terminal","text":"Ping the server using its IPv4 address: ping 162.55.178.186 Find the IP under Servers \u2192 Select your server (secure-server). Connect via SSH: ssh root@162.55.178.186","title":"Step 4: Test the Server in the Terminal"},{"location":"ex3/#fix-known-hosts-error","text":"If you get a host key mismatch error: List your hidden .ssh directory: ls -al ~/.ssh 2.Open the known_hosts file: nano ~/.ssh/known_hosts Remove all entries for this server (usually the last three). Tip for Mac: Install micro for easier file editing: brew install micro Retry the SSH login after saving the file. Example root password (do not use in real setup): xTixqiReWkaLif3qhnN3","title":"Fix Known Hosts Error"},{"location":"ex3/#step-5-update-reboot","text":"Update all packages: apt update && apt upgrade -y Reboot the server: reboot Log in again via SSH.","title":"Step 5: Update &amp; Reboot"},{"location":"ex3/#step-6-install-test-nginx","text":"Install Nginx: apt install nginx -y Check status: systemctl status nginx If you see active (running) \u2192 Nginx is working. Test locally: wget -O - http://162.55.178.186 If you see HTML with \u201cWelcome to nginx!\u201d, it\u2019s running.","title":"Step 6: Install &amp; Test Nginx"},{"location":"ex3/#step-7-enable-browser-access","text":"Open in your browser: http://162.55.178.186 If it doesn\u2019t load, the firewall is blocking port 80. Fix the firewall: Go to Firewalls \u2192 secure-firewall \u2192 Edit. Add a new inbound rule: Direction: In Protocol: TCP Port: 80 Save the changes. Test again in your browser. You should now see Welcome to nginx!.","title":"Step 7: Enable Browser Access"},{"location":"ex4/","text":"Excersise 4: SSH-Agent Installation Step 1: Check if ssh-agent is Installed Run: which ssh-agent If it shows a path like /usr/bin/ssh-agent, it\u2019s installed. If not, install it: sudo apt install openssh-client Step 2: Start ssh-agent Start the agent in the background: eval \"$(ssh-agent -s)\" This sets environment variables so your shell can communicate with it. Step 3: Add Your Private Key ssh-add ~/.ssh/id_rsa (Use the private key file corresponding to the public key you uploaded earlier.) Step 4: Test with Multiple SSH Connections Connect to your server as before: ssh root@162.55.178.186 The ssh-agent will handle the key, so you won\u2019t need to re-enter it for subseque-nt connections.","title":"Exercise 4"},{"location":"ex4/#excersise-4-ssh-agent-installation","text":"","title":"Excersise 4: SSH-Agent Installation"},{"location":"ex4/#step-1-check-if-ssh-agent-is-installed","text":"Run: which ssh-agent If it shows a path like /usr/bin/ssh-agent, it\u2019s installed. If not, install it: sudo apt install openssh-client","title":"Step 1: Check if ssh-agent is Installed"},{"location":"ex4/#step-2-start-ssh-agent","text":"Start the agent in the background: eval \"$(ssh-agent -s)\" This sets environment variables so your shell can communicate with it.","title":"Step 2: Start ssh-agent"},{"location":"ex4/#step-3-add-your-private-key","text":"ssh-add ~/.ssh/id_rsa (Use the private key file corresponding to the public key you uploaded earlier.)","title":"Step 3: Add Your Private Key"},{"location":"ex4/#step-4-test-with-multiple-ssh-connections","text":"Connect to your server as before: ssh root@162.55.178.186 The ssh-agent will handle the key, so you won\u2019t need to re-enter it for subseque-nt connections.","title":"Step 4: Test with Multiple SSH Connections"},{"location":"ex5/","text":"Excersise 5: MI Gitlab access by ssh Step 1: Add SSH Key to GitLab Go to your User Settings and open the SSH Keys tab. Add your public key there. You can test if the SSH key works by running: ssh -T git@gitlab.mi.hdm-stuttgart.de You should see the message: Welcome to GitLab, <user>! Step 2: Create and Clone a Test Project Create an empty test project in GitLab. Copy the SSH clone link from the Clone option. You can then clone the project with: git clone git@gitlab.mi.hdm-stuttgart.de:<user>/<projectName>.git Step 3: Test Git Functions Now you can make simple changes, e.g., edit the README.md . Add the changes using git add , commit them, and then test push and pull.","title":"Exercise 5"},{"location":"ex5/#excersise-5-mi-gitlab-access-by-ssh","text":"","title":"Excersise 5: MI Gitlab access by ssh"},{"location":"ex5/#step-1-add-ssh-key-to-gitlab","text":"Go to your User Settings and open the SSH Keys tab. Add your public key there. You can test if the SSH key works by running: ssh -T git@gitlab.mi.hdm-stuttgart.de You should see the message: Welcome to GitLab, <user>!","title":"Step 1: Add SSH Key to GitLab"},{"location":"ex5/#step-2-create-and-clone-a-test-project","text":"Create an empty test project in GitLab. Copy the SSH clone link from the Clone option. You can then clone the project with: git clone git@gitlab.mi.hdm-stuttgart.de:<user>/<projectName>.git","title":"Step 2: Create and Clone a Test Project"},{"location":"ex5/#step-3-test-git-functions","text":"Now you can make simple changes, e.g., edit the README.md . Add the changes using git add , commit them, and then test push and pull.","title":"Step 3: Test Git Functions"},{"location":"ex6/","text":"Excersise 6: SSH Host Hopping Preparation: First, create two Hetzner servers: Host A and Host B. Host A: Has a public IP address Uses the same SSH key as Host B Shares a private network with Host B Host B: Has only a private network IP Uses the same SSH key as Host A Start the SSH agent and load the private key. Step 1: Connect to Host A with Agent Forwarding and Jump to Host B Connect to Host A using: ssh -A root@<ip-host-a> The local key is forwarded via the agent, otherwise it wouldn\u2019t be recognized. Once on Host A, connect to Host B using: ssh root@<network-ip-host-b> Step 2: Close Both Connections Exit both sessions with: exit Step 3: Try Connecting to Host B Directly from Local If you try to connect directly from your local machine to Host B, it should fail and return an error. Host B is not supposed to be directly accessible . Step 4: Attempt to Connect Back to Host A from Host B If you connect from Host A to Host B and then try to connect back from Host B to Host A, this should also fail. Agent forwarding only works one way, so Host B does not have access to your local key.","title":"Exercise 6"},{"location":"ex6/#excersise-6-ssh-host-hopping","text":"","title":"Excersise 6: SSH Host Hopping"},{"location":"ex6/#preparation","text":"First, create two Hetzner servers: Host A and Host B. Host A: Has a public IP address Uses the same SSH key as Host B Shares a private network with Host B Host B: Has only a private network IP Uses the same SSH key as Host A Start the SSH agent and load the private key.","title":"Preparation:"},{"location":"ex6/#step-1-connect-to-host-a-with-agent-forwarding-and-jump-to-host-b","text":"Connect to Host A using: ssh -A root@<ip-host-a> The local key is forwarded via the agent, otherwise it wouldn\u2019t be recognized. Once on Host A, connect to Host B using: ssh root@<network-ip-host-b>","title":"Step 1: Connect to Host A with Agent Forwarding and Jump to Host B"},{"location":"ex6/#step-2-close-both-connections","text":"Exit both sessions with: exit","title":"Step 2: Close Both Connections"},{"location":"ex6/#step-3-try-connecting-to-host-b-directly-from-local","text":"If you try to connect directly from your local machine to Host B, it should fail and return an error. Host B is not supposed to be directly accessible .","title":"Step 3: Try Connecting to Host B Directly from Local"},{"location":"ex6/#step-4-attempt-to-connect-back-to-host-a-from-host-b","text":"If you connect from Host A to Host B and then try to connect back from Host B to Host A, this should also fail. Agent forwarding only works one way, so Host B does not have access to your local key.","title":"Step 4: Attempt to Connect Back to Host A from Host B"},{"location":"ex7/","text":"Excersise 7: SSH Port Forwarding new server is created with a firewall that has two rules: Port 22 with TCP ICMP protocol This means Port 80 is restricted by default. Step 1: Prepare the Server Log into the server via bash and run the following commands to install and start Nginx: apt update apt install nginx systemctl start nginx Step 2: Check if Nginx is Reachable from Outside On your local machine (not inside the server), run: curl http://<server-ip> You should receive the Nginx default HTML page. Step 3: Set Up SSH Port Forwarding Forward Port 80 on the server to your local Port 2000 via SSH: ssh -L 2000:localhost:80 root@<server-ip> Step 4: Test in the Browser While the SSH connection is active, open your browser and go to: http://localhost:2000 You should see the same Nginx welcome page. Once the SSH connection is closed, Nginx will no longer be accessible through the forwarded port.","title":"Exercise 7"},{"location":"ex7/#excersise-7-ssh-port-forwarding","text":"new server is created with a firewall that has two rules: Port 22 with TCP ICMP protocol This means Port 80 is restricted by default.","title":"Excersise 7: SSH Port Forwarding"},{"location":"ex7/#step-1-prepare-the-server","text":"Log into the server via bash and run the following commands to install and start Nginx: apt update apt install nginx systemctl start nginx","title":"Step 1: Prepare the Server"},{"location":"ex7/#step-2-check-if-nginx-is-reachable-from-outside","text":"On your local machine (not inside the server), run: curl http://<server-ip> You should receive the Nginx default HTML page.","title":"Step 2: Check if Nginx is Reachable from Outside"},{"location":"ex7/#step-3-set-up-ssh-port-forwarding","text":"Forward Port 80 on the server to your local Port 2000 via SSH: ssh -L 2000:localhost:80 root@<server-ip>","title":"Step 3: Set Up SSH Port Forwarding"},{"location":"ex7/#step-4-test-in-the-browser","text":"While the SSH connection is active, open your browser and go to: http://localhost:2000 You should see the same Nginx welcome page. Once the SSH connection is closed, Nginx will no longer be accessible through the forwarded port.","title":"Step 4: Test in the Browser"},{"location":"ex8/","text":"Excersise 8: SSH X11 Forwarding Step 1: Setup For this task, we use the same server as in No. 7, since the settings remain the same. Step 2: X11 Client for Windows At first, we tried with XLaunch . The connection worked, but when attempting to open Firefox, the following error appeared. After searching online, I found that there are known issues with the combination of Bash, Windows, and XLaunch. So instead, I tried with another program: MobaXterm . Click on New Session Enter the server\u2019s IP in Remote Host Check Specify username and enter root Ensure X11 Forwarding is selected (it already was by default) Activate Use private key and select your key file Step 3: Start Firefox After clicking OK, the server connects directly. Then simply run: firefox & to start Firefox. Step 4: Open http://localhost In the Firefox browser, open: http://localhost You should see the \u201cWelcome to nginx!\u201d page again.","title":"Exercise 8"},{"location":"ex8/#excersise-8-ssh-x11-forwarding","text":"","title":"Excersise 8: SSH X11 Forwarding"},{"location":"ex8/#step-1-setup","text":"For this task, we use the same server as in No. 7, since the settings remain the same.","title":"Step 1: Setup"},{"location":"ex8/#step-2-x11-client-for-windows","text":"At first, we tried with XLaunch . The connection worked, but when attempting to open Firefox, the following error appeared. After searching online, I found that there are known issues with the combination of Bash, Windows, and XLaunch. So instead, I tried with another program: MobaXterm . Click on New Session Enter the server\u2019s IP in Remote Host Check Specify username and enter root Ensure X11 Forwarding is selected (it already was by default) Activate Use private key and select your key file","title":"Step 2: X11 Client for Windows"},{"location":"ex8/#step-3-start-firefox","text":"After clicking OK, the server connects directly. Then simply run: firefox & to start Firefox.","title":"Step 3: Start Firefox"},{"location":"ex8/#step-4-open-httplocalhost","text":"In the Firefox browser, open: http://localhost You should see the \u201cWelcome to nginx!\u201d page again.","title":"Step 4: Open http://localhost"},{"location":"ex9/","text":"Excersise 9: Enabling index based file search Step 1: Setup We continue using the server from Task 7. Install plocate with: apt install plocate Step 2: Create an Index of the Current Filesystem Structure In order to find files, you first need to run: updatedb This records the filesystem structure. As a test, search for \u201captitude\u201d with: locate aptitude Step 3: Test with a New File Create a new file and search for it immediately. Since updatedb was not executed beforehand, the file cannot be found. After running updatedb , the file localtest is found. If you delete the file and search for it again without running updatedb , it will still appear in the results, because the filesystem index was not updated.","title":"Exercise 9"},{"location":"ex9/#excersise-9-enabling-index-based-file-search","text":"","title":"Excersise 9: Enabling index based file search"},{"location":"ex9/#step-1-setup","text":"We continue using the server from Task 7. Install plocate with: apt install plocate","title":"Step 1: Setup"},{"location":"ex9/#step-2-create-an-index-of-the-current-filesystem-structure","text":"In order to find files, you first need to run: updatedb This records the filesystem structure. As a test, search for \u201captitude\u201d with: locate aptitude","title":"Step 2: Create an Index of the Current Filesystem Structure"},{"location":"ex9/#step-3-test-with-a-new-file","text":"Create a new file and search for it immediately. Since updatedb was not executed beforehand, the file cannot be found. After running updatedb , the file localtest is found. If you delete the file and search for it again without running updatedb , it will still appear in the results, because the filesystem index was not updated.","title":"Step 3: Test with a New File"}]}