{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"ex1/","text":"","title":"Exercise 1"},{"location":"ex10/","text":"","title":"Exercise 10"},{"location":"ex11/","text":"Excersise 11: Creating a Basic Server with Terraform on Hetzner Cloud 1. Minimal Terraform Configuration We started from a minimal Terraform configuration (based on Figure 1004) and defined a Hetzner Cloud server resource. The provider was initialized using the Hetzner API token. Steps: Created a new Terraform project. Initialized the backend using: terraform init Confirmation: Terraform has been successfully initialized! Defined a hcloud_server resource ( exercise-11 ) with: Debian 12 image cpx11 server type 40 GB disk Firewall configuration SSH key access. 2. Firewall Rule for SSH Access An inbound firewall rule was added to allow SSH (port 22) connections. This was necessary to establish secure remote access to the server. 3. Hetzner API Token We entered the Hetzner Cloud API token to enable Terraform to authenticate with Hetzner. Note: Since version control is required, storing the token directly in the configuration would be unsafe. Instead, we followed the solution from Figure 1018 by placing the token in a separate file (not committed to Git). 4. Server Creation and Initial Login Running: terraform apply Terraform created the server successfully: hcloud_server.exercise_11: Creation complete after 13s Hetzner automatically sent an email with the server\u2019s IP address and root password. This happens because no SSH key was yet associated with the server at the time of its first creation. For security, Hetzner provides initial credentials via email. We verified login via: ssh root@<server-ip> 5. Switching to Public/Private Key Authentication To avoid insecure password-based login, we configured SSH key authentication. Steps: Created an hcloud_ssh_key resource with our public key. Attached the SSH key to the server resource in Terraform. Reapplied Terraform configuration. Verified login with: ssh -i ~/.ssh/id_rsa root@<server-ip> Now login works securely with our private key. 6. Outputs Configuration To automatically display the server IP and datacenter, we added an outputs.tf file with: output \"server_ip\" { value = hcloud_server.exercise_11.ipv4_address } output \"server_datacenter\" { value = hcloud_server.exercise_11.datacenter } On success, the output looked like: Apply complete! Resources: 3 added, 0 changed, 0 destroyed. Outputs: server_datacenter = \"hel1-dc2\" server_ip = \"95.216.223.223\" 7. Version Control with Git The Terraform configuration was placed under version control in a Git repository. To avoid leaking secrets, the Hetzner API token was excluded by using .gitignore and a separate file approach.","title":"Exercise 11"},{"location":"ex11/#excersise-11-creating-a-basic-server-with-terraform-on-hetzner-cloud","text":"","title":"Excersise 11: Creating a Basic Server with Terraform on Hetzner Cloud"},{"location":"ex11/#1-minimal-terraform-configuration","text":"We started from a minimal Terraform configuration (based on Figure 1004) and defined a Hetzner Cloud server resource. The provider was initialized using the Hetzner API token. Steps: Created a new Terraform project. Initialized the backend using: terraform init Confirmation: Terraform has been successfully initialized! Defined a hcloud_server resource ( exercise-11 ) with: Debian 12 image cpx11 server type 40 GB disk Firewall configuration SSH key access.","title":"1. Minimal Terraform Configuration"},{"location":"ex11/#2-firewall-rule-for-ssh-access","text":"An inbound firewall rule was added to allow SSH (port 22) connections. This was necessary to establish secure remote access to the server.","title":"2. Firewall Rule for SSH Access"},{"location":"ex11/#3-hetzner-api-token","text":"We entered the Hetzner Cloud API token to enable Terraform to authenticate with Hetzner. Note: Since version control is required, storing the token directly in the configuration would be unsafe. Instead, we followed the solution from Figure 1018 by placing the token in a separate file (not committed to Git).","title":"3. Hetzner API Token"},{"location":"ex11/#4-server-creation-and-initial-login","text":"Running: terraform apply Terraform created the server successfully: hcloud_server.exercise_11: Creation complete after 13s Hetzner automatically sent an email with the server\u2019s IP address and root password. This happens because no SSH key was yet associated with the server at the time of its first creation. For security, Hetzner provides initial credentials via email. We verified login via: ssh root@<server-ip>","title":"4. Server Creation and Initial Login"},{"location":"ex11/#5-switching-to-publicprivate-key-authentication","text":"To avoid insecure password-based login, we configured SSH key authentication. Steps: Created an hcloud_ssh_key resource with our public key. Attached the SSH key to the server resource in Terraform. Reapplied Terraform configuration. Verified login with: ssh -i ~/.ssh/id_rsa root@<server-ip> Now login works securely with our private key.","title":"5. Switching to Public/Private Key Authentication"},{"location":"ex11/#6-outputs-configuration","text":"To automatically display the server IP and datacenter, we added an outputs.tf file with: output \"server_ip\" { value = hcloud_server.exercise_11.ipv4_address } output \"server_datacenter\" { value = hcloud_server.exercise_11.datacenter } On success, the output looked like: Apply complete! Resources: 3 added, 0 changed, 0 destroyed. Outputs: server_datacenter = \"hel1-dc2\" server_ip = \"95.216.223.223\"","title":"6. Outputs Configuration"},{"location":"ex11/#7-version-control-with-git","text":"The Terraform configuration was placed under version control in a Git repository. To avoid leaking secrets, the Hetzner API token was excluded by using .gitignore and a separate file approach.","title":"7. Version Control with Git"},{"location":"ex12/","text":"Excersise 12: Automatic Nginx Installation Using Terraform and Cloud-init 1. Objective The goal of this exercise was to automatically install and configure the Nginx web server on a newly created Hetzner Cloud instance using a user_data script in Terraform. Additionally, SSH password login and root login were disabled, and a dedicated user was created for secure access. 2. Implementation Steps 2.1 Terraform Configuration with Cloud-init A user_data script was integrated into the Terraform configuration. This script was executed during server initialization and performed the following actions: 1. Installed Nginx Used the package manager ( apt-get ) to install the Nginx web server. 2. Started the Nginx Service Activated the service immediately with: systemctl start nginx 3. Enabled Nginx on Boot Configured Nginx to start automatically after server restarts: systemctl enable nginx 2.2 User and Security Configuration The Cloud-init script included additional security hardening: Disabled SSH Password Login ssh_pwauth: false Disabled Root Login disable_root: true Created a New User ( devops ) - Added to the sudo group. - Configured with SSH key authentication. - Verified login with: hcl ssh devops@<server-ip> Attempted root login now returns: \u201cPlease login as the user NONE\u201d , confirming root access is blocked. 2.3 Verification of Nginx After the server was created and initialized, accessing the server\u2019s public IP in a browser displayed the default Nginx landing page: Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. This confirmed that the automated installation and configuration were successful. 3. Outputs The Terraform outputs.tf file was used to display relevant information: output \"server_ip\" { value = hcloud_server.exercise_11.ipv4_address } output \"server_datacenter\" { value = hcloud_server.exercise_11.datacenter } Result after apply: Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: server_datacenter = \"hel1-dc2\" server_ip = \"95.216.223.223\"","title":"Exercise 12"},{"location":"ex12/#excersise-12-automatic-nginx-installation-using-terraform-and-cloud-init","text":"","title":"Excersise 12: Automatic Nginx Installation Using Terraform and Cloud-init"},{"location":"ex12/#1-objective","text":"The goal of this exercise was to automatically install and configure the Nginx web server on a newly created Hetzner Cloud instance using a user_data script in Terraform. Additionally, SSH password login and root login were disabled, and a dedicated user was created for secure access.","title":"1. Objective"},{"location":"ex12/#2-implementation-steps","text":"2.1 Terraform Configuration with Cloud-init A user_data script was integrated into the Terraform configuration. This script was executed during server initialization and performed the following actions: 1. Installed Nginx Used the package manager ( apt-get ) to install the Nginx web server. 2. Started the Nginx Service Activated the service immediately with: systemctl start nginx 3. Enabled Nginx on Boot Configured Nginx to start automatically after server restarts: systemctl enable nginx","title":"2. Implementation Steps"},{"location":"ex12/#22-user-and-security-configuration","text":"The Cloud-init script included additional security hardening: Disabled SSH Password Login ssh_pwauth: false Disabled Root Login disable_root: true Created a New User ( devops ) - Added to the sudo group. - Configured with SSH key authentication. - Verified login with: hcl ssh devops@<server-ip> Attempted root login now returns: \u201cPlease login as the user NONE\u201d , confirming root access is blocked.","title":"2.2 User and Security Configuration"},{"location":"ex12/#23-verification-of-nginx","text":"After the server was created and initialized, accessing the server\u2019s public IP in a browser displayed the default Nginx landing page: Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. This confirmed that the automated installation and configuration were successful.","title":"2.3 Verification of Nginx"},{"location":"ex12/#3-outputs","text":"The Terraform outputs.tf file was used to display relevant information: output \"server_ip\" { value = hcloud_server.exercise_11.ipv4_address } output \"server_datacenter\" { value = hcloud_server.exercise_11.datacenter } Result after apply: Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: server_datacenter = \"hel1-dc2\" server_ip = \"95.216.223.223\"","title":"3. Outputs"},{"location":"ex13/","text":"Excersise 13: Working on Cloud-init with Terraform 1. Objective The goal of this exercise was to extend our base system deployment by using Cloud-init with Terraform to automatically configure a secure web server. Key requirements were: Deploy an Nginx web server with a custom landing page. Enable inbound HTTP traffic (port 80) via firewall configuration. Harden SSH access (no password login, no root login). Create a devops user with SSH key and sudo privileges. Upgrade the Debian 12 distribution during server creation. Install and configure fail2ban for protection against repeated failed SSH login attempts. Install and initialize the plocate file indexer. 2. Terraform and Cloud-init Configuration 2.1 Firewall Setup The firewall configuration allowed inbound traffic for: SSH (TCP port 22) for secure administration. HTTP (TCP port 80) for Nginx. Example snippet: resource \"hcloud_firewall\" \"exercise_13_firewall\" { name = \"exercise-13-firewall\" rule { direction = \"in\" protocol = \"tcp\" port = \"22\" source_ips = [\"0.0.0.0/0\", \"::/0\"] description = \"SSH inbound\" } rule { direction = \"in\" protocol = \"tcp\" port = \"80\" source_ips = [\"0.0.0.0/0\", \"::/0\"] description = \"HTTP inbound\" } } 2.2 Cloud-init User Data Script A Cloud-init script was included in the Terraform configuration. It performed the following tasks automatically at server creation: Installed and started Nginx, and enabled it to start on reboot. Updated and upgraded all Debian packages. Disabled SSH password authentication. Disabled root login. Created user devops with sudo privileges and SSH key access. Installed and configured fail2ban. Installed and initialized plocate for fast file searching. 3. Security and Access Configuration Password login disabled Configured in /etc/ssh/sshd_config : PasswordAuthentication no PermitRootLogin no Root login disabled Attempting ssh root@<ip> now returns: Please login as the user \"NONE\" rather than the user \"root\". Created devops user Login works via: ssh devops@<server-ip> Sudo access confirmed The devops user can switch to root: sudo su - 4. Verification of the Web Server After deployment, visiting the server\u2019s IP in a browser displayed the custom Nginx landing page: I'm Nginx @ \"37.27.219.19\" created Wed Jul 30 11:14:38 PM UTC 2025 This confirmed that: Nginx was installed correctly. The firewall allowed inbound traffic on port 80. The landing page was deployed via Cloud-init. 5. System Update at Creation The Cloud-init script updated and upgraded the Debian 12 system packages automatically during initialization. Verification with: sudo apt update Output confirmed: All packages are up to date. 6. Fail2ban Configuration Fail2ban was installed and configured to monitor SSH login attempts. Verification command: sudo fail2ban-client status sshd```` Expected output includes information about failed login attempts and banned IP addresses: ```hcl |- Currently banned: 2 |- Total banned: 2 `- Banned IP list: 170.64.133.30 213.136.94.219 7. Plocate Installation The plocate package was installed and initialized to enable fast file searches. Tested with: locate ssh_host Output listed SSH host key files: /etc/ssh/ssh_host_ed25519_key /etc/ssh/ssh_host_ed25519_key.pub","title":"Exercise 13"},{"location":"ex13/#excersise-13-working-on-cloud-init-with-terraform","text":"","title":"Excersise 13: Working on Cloud-init with Terraform"},{"location":"ex13/#1-objective","text":"The goal of this exercise was to extend our base system deployment by using Cloud-init with Terraform to automatically configure a secure web server. Key requirements were: Deploy an Nginx web server with a custom landing page. Enable inbound HTTP traffic (port 80) via firewall configuration. Harden SSH access (no password login, no root login). Create a devops user with SSH key and sudo privileges. Upgrade the Debian 12 distribution during server creation. Install and configure fail2ban for protection against repeated failed SSH login attempts. Install and initialize the plocate file indexer.","title":"1. Objective"},{"location":"ex13/#2-terraform-and-cloud-init-configuration","text":"2.1 Firewall Setup The firewall configuration allowed inbound traffic for: SSH (TCP port 22) for secure administration. HTTP (TCP port 80) for Nginx. Example snippet: resource \"hcloud_firewall\" \"exercise_13_firewall\" { name = \"exercise-13-firewall\" rule { direction = \"in\" protocol = \"tcp\" port = \"22\" source_ips = [\"0.0.0.0/0\", \"::/0\"] description = \"SSH inbound\" } rule { direction = \"in\" protocol = \"tcp\" port = \"80\" source_ips = [\"0.0.0.0/0\", \"::/0\"] description = \"HTTP inbound\" } } 2.2 Cloud-init User Data Script A Cloud-init script was included in the Terraform configuration. It performed the following tasks automatically at server creation: Installed and started Nginx, and enabled it to start on reboot. Updated and upgraded all Debian packages. Disabled SSH password authentication. Disabled root login. Created user devops with sudo privileges and SSH key access. Installed and configured fail2ban. Installed and initialized plocate for fast file searching.","title":"2. Terraform and Cloud-init Configuration"},{"location":"ex13/#3-security-and-access-configuration","text":"Password login disabled Configured in /etc/ssh/sshd_config : PasswordAuthentication no PermitRootLogin no Root login disabled Attempting ssh root@<ip> now returns: Please login as the user \"NONE\" rather than the user \"root\". Created devops user Login works via: ssh devops@<server-ip> Sudo access confirmed The devops user can switch to root: sudo su -","title":"3. Security and Access Configuration"},{"location":"ex13/#4-verification-of-the-web-server","text":"After deployment, visiting the server\u2019s IP in a browser displayed the custom Nginx landing page: I'm Nginx @ \"37.27.219.19\" created Wed Jul 30 11:14:38 PM UTC 2025 This confirmed that: Nginx was installed correctly. The firewall allowed inbound traffic on port 80. The landing page was deployed via Cloud-init.","title":"4. Verification of the Web Server"},{"location":"ex13/#5-system-update-at-creation","text":"The Cloud-init script updated and upgraded the Debian 12 system packages automatically during initialization. Verification with: sudo apt update Output confirmed: All packages are up to date.","title":"5. System Update at Creation"},{"location":"ex13/#6-fail2ban-configuration","text":"Fail2ban was installed and configured to monitor SSH login attempts. Verification command: sudo fail2ban-client status sshd```` Expected output includes information about failed login attempts and banned IP addresses: ```hcl |- Currently banned: 2 |- Total banned: 2 `- Banned IP list: 170.64.133.30 213.136.94.219","title":"6. Fail2ban Configuration"},{"location":"ex13/#7-plocate-installation","text":"The plocate package was installed and initialized to enable fast file searches. Tested with: locate ssh_host Output listed SSH host key files: /etc/ssh/ssh_host_ed25519_key /etc/ssh/ssh_host_ed25519_key.pub","title":"7. Plocate Installation"},{"location":"ex14/","text":"Exercise 14: Solving the ~/.ssh/known_hosts Quirk 1. Objective The aim of this exercise was to resolve the recurring problem of SSH host key verification warnings (or failures) when provisioning new servers via Terraform. The solution required automatically generating a known_hosts file and wrapper scripts for ssh and scp to ensure smooth, secure access without manual confirmations. 2. Project Setup A new project folder Excercice_14 was created with the following structure: Excercice_14/ tpl/ # Template scripts for ssh/scp bin/ # Generated wrapper scripts gen/ # Generated SSH keys and knwn_hosts main.tf # Terraform configuration outputs.tf # Terraform outputs cloud_init.yml 3. Templates for Wrapper Scripts 3.1 SSH Wrapper Template (tpl/ssh.sh) #!/usr/bin/env bash GEN_DIR=$(dirname \"$0\")/../gen ssh -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" ${devopsUsername}@${ip} \"$@\" ${devopsUsername} and ${ip} are replaced by Terraform using templatefile(). The \"$@\" allows optional additional SSH arguments. 3.2 SCP Wrapper Template ( tpl/scp.sh ) #!/usr/bin/env bash GEN_DIR=$(dirname \"$0\")/../gen if [ $# -lt 2 ]; then echo \"usage: .../bin/scp <source> <destination>\" else scp -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" \"$@\" fi Ensures at least two arguments are passed (source and destination). Uses the same custom known_hosts file as the SSH wrapper. 4. Terraform Configuration 4.1 Providers and Keys Added the tls and local providers in addition to hetznercloud/hcloud. Generated an ED25519 key pair via Terraform: resource \"tls_private_key\" \"devops_key\" { algorithm = \"ED25519\" } Stored the keys in the gen/ directory. Registered the public key in Hetzner with: resource \"hcloud_ssh_key\" \"devops_key\" { name = \"exercise14-key\" public_key = tls_private_key.devops_key.public_key_openssh } 4.2 Known Hosts File Terraform generated a gen/known_hosts file containing the server IP and host key fingerprint. This file ensures SSH does not show host key verification warnings. 4.3 Server Creation Created a Debian 12 server with Cloud-init. Attached the devops SSH key. Added firewall rules for SSH (22) and HTTP (80). 5. Cloud-init Configuration The Cloud-init script was reused from Exercise 13 and ensured: Installation and enabling of Nginx. Creation of a devops user with sudo privileges. Disabled password authentication ( ssh_pwauth: false ). Disabled root login ( disable_root: true ). 6. Testing and Verification 6.1 Terraform Apply Execution of: terraform apply Output confirmed: Apply complete! Resources: 9 added, 0 changed, 0 destroyed. Outputs: server_datacenter = \"hel1-dc2\" server_ip = \"95.216.223.223\" 6.2 Using the SSH Wrapper ./bin/ssh Successfully logged into the server as devops. No host key warnings appeared. 6.3 File Transfer with SCP Wrapper echo \"Hello from Pauline\" > localfile.txt ./bin/scp localfile.txt devops@95.216.223.223:/home/devops/ On the server: ls -l /home/devops/ cat /home/devops/localfile.txt Output: Hello from Pauline Confirmed that file transfer works correctly.","title":"Exercise 14"},{"location":"ex14/#exercise-14-solving-the-sshknown_hosts-quirk","text":"","title":"Exercise 14: Solving the ~/.ssh/known_hosts Quirk"},{"location":"ex14/#1-objective","text":"The aim of this exercise was to resolve the recurring problem of SSH host key verification warnings (or failures) when provisioning new servers via Terraform. The solution required automatically generating a known_hosts file and wrapper scripts for ssh and scp to ensure smooth, secure access without manual confirmations.","title":"1. Objective"},{"location":"ex14/#2-project-setup","text":"A new project folder Excercice_14 was created with the following structure: Excercice_14/ tpl/ # Template scripts for ssh/scp bin/ # Generated wrapper scripts gen/ # Generated SSH keys and knwn_hosts main.tf # Terraform configuration outputs.tf # Terraform outputs cloud_init.yml","title":"2. Project Setup"},{"location":"ex14/#3-templates-for-wrapper-scripts","text":"3.1 SSH Wrapper Template (tpl/ssh.sh) #!/usr/bin/env bash GEN_DIR=$(dirname \"$0\")/../gen ssh -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" ${devopsUsername}@${ip} \"$@\" ${devopsUsername} and ${ip} are replaced by Terraform using templatefile(). The \"$@\" allows optional additional SSH arguments.","title":"3. Templates for Wrapper Scripts"},{"location":"ex14/#32-scp-wrapper-template-tplscpsh","text":"#!/usr/bin/env bash GEN_DIR=$(dirname \"$0\")/../gen if [ $# -lt 2 ]; then echo \"usage: .../bin/scp <source> <destination>\" else scp -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" \"$@\" fi Ensures at least two arguments are passed (source and destination). Uses the same custom known_hosts file as the SSH wrapper.","title":"3.2 SCP Wrapper Template (tpl/scp.sh)"},{"location":"ex14/#4-terraform-configuration","text":"4.1 Providers and Keys Added the tls and local providers in addition to hetznercloud/hcloud. Generated an ED25519 key pair via Terraform: resource \"tls_private_key\" \"devops_key\" { algorithm = \"ED25519\" } Stored the keys in the gen/ directory. Registered the public key in Hetzner with: resource \"hcloud_ssh_key\" \"devops_key\" { name = \"exercise14-key\" public_key = tls_private_key.devops_key.public_key_openssh } 4.2 Known Hosts File Terraform generated a gen/known_hosts file containing the server IP and host key fingerprint. This file ensures SSH does not show host key verification warnings. 4.3 Server Creation Created a Debian 12 server with Cloud-init. Attached the devops SSH key. Added firewall rules for SSH (22) and HTTP (80).","title":"4. Terraform Configuration"},{"location":"ex14/#5-cloud-init-configuration","text":"The Cloud-init script was reused from Exercise 13 and ensured: Installation and enabling of Nginx. Creation of a devops user with sudo privileges. Disabled password authentication ( ssh_pwauth: false ). Disabled root login ( disable_root: true ).","title":"5. Cloud-init Configuration"},{"location":"ex14/#6-testing-and-verification","text":"6.1 Terraform Apply Execution of: terraform apply Output confirmed: Apply complete! Resources: 9 added, 0 changed, 0 destroyed. Outputs: server_datacenter = \"hel1-dc2\" server_ip = \"95.216.223.223\" 6.2 Using the SSH Wrapper ./bin/ssh Successfully logged into the server as devops. No host key warnings appeared. 6.3 File Transfer with SCP Wrapper echo \"Hello from Pauline\" > localfile.txt ./bin/scp localfile.txt devops@95.216.223.223:/home/devops/ On the server: ls -l /home/devops/ cat /home/devops/localfile.txt Output: Hello from Pauline Confirmed that file transfer works correctly.","title":"6. Testing and Verification"},{"location":"ex15/","text":"Exercise 15: Partitions and Mounting 1. Objective The goal of this exercise was to extend our Terraform-managed server by attaching an additional volume, then manually partitioning and mounting it inside the Linux system. Finally, the setup had to ensure that the mounts persisted across reboots. 2. Terraform Configuration We extended our Terraform configuration to create a new Hetzner Cloud volume and attach it to the server provisioned in Exercise 14. Volume Resource resource \"hcloud_volume\" \"volume01\" { name = \"volume1\" size = 10 server_id = hcloud_server.exercise_14.id automount = true format = \"xfs\" } Outputs output \"volume_id\" { value = hcloud_volume.volume01.id description = \"The volume's id\" } output \"volume_linux_device\" { value = hcloud_volume.volume01.linux_device description = \"The volume's linux device\" } After applying: Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: server_datacenter = \"hel1-dc2\" server_ip = \"95.216.223.223\" volume_id = \"102995739\" volume_linux_device = \"/dev/disk/by-id/scsi-0HC_Volume_102995739\" 3. Detecting the New Volume Initially, the new volume /dev/sdb was not visible. To detect it, we ran: sudo udevadm trigger Afterwards, listing block devices confirmed its presence: lsblk Output (before partitioning): sdb 8:16 0 10G 0 disk 4. Partitioning the Volume We used fdisk to create two primary partitions of ~5 GB each: sudo fdisk /dev/sdb Steps: n \u2192 create new partition Type: p \u2192 primary Size: +5G Repeated for second partition Saved with w Resulting devices: /dev/sdb1 5G /dev/sdb2 5G 5. Creating File Systems We formatted the new partitions with different file systems: sudo mkfs -t ext4 /dev/sdb1 sudo mkfs -t xfs /dev/sdb2 6. Mounting the Partitions We created mount points: sudo mkdir /disk1 /disk2 Mounted them as follows: sudo mount /dev/sdb1 /disk1 sudo mount UUID=$(sudo blkid -s UUID -o value /dev/sdb2) /disk2 Verification: df -h Showed /disk1 and /disk2 mounted successfully. 7. Persistent Mounts with fstab We edited /etc/fstab to ensure partitions mount automatically after reboot. Entries added: /dev/sdb1 /disk1 ext4 defaults 0 0 UUID=<UUID-of-sdb2> /disk2 xfs defaults 0 0 Then tested with: sudo mount -a No errors occurred, confirming the syntax was valid. 8. Verification After Reboot After rebooting the system, both partitions were automatically mounted. df -h Confirmed that /disk1 (ext4) and /disk2 (xfs) were available. 9. Problems Encountered Volume not visible at first \u2192 Solved with sudo udevadm trigger . fstab UUID mistake \u2192 Accidentally typed placeholder instead of real UUID. Corrected using sudo blkid . fstab syntax caution \u2192 Verified with mount -a before reboot to avoid boot issues.","title":"Exercise 15"},{"location":"ex15/#exercise-15-partitions-and-mounting","text":"","title":"Exercise 15: Partitions and Mounting"},{"location":"ex15/#1-objective","text":"The goal of this exercise was to extend our Terraform-managed server by attaching an additional volume, then manually partitioning and mounting it inside the Linux system. Finally, the setup had to ensure that the mounts persisted across reboots.","title":"1. Objective"},{"location":"ex15/#2-terraform-configuration","text":"We extended our Terraform configuration to create a new Hetzner Cloud volume and attach it to the server provisioned in Exercise 14. Volume Resource resource \"hcloud_volume\" \"volume01\" { name = \"volume1\" size = 10 server_id = hcloud_server.exercise_14.id automount = true format = \"xfs\" } Outputs output \"volume_id\" { value = hcloud_volume.volume01.id description = \"The volume's id\" } output \"volume_linux_device\" { value = hcloud_volume.volume01.linux_device description = \"The volume's linux device\" } After applying: Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: server_datacenter = \"hel1-dc2\" server_ip = \"95.216.223.223\" volume_id = \"102995739\" volume_linux_device = \"/dev/disk/by-id/scsi-0HC_Volume_102995739\"","title":"2. Terraform Configuration"},{"location":"ex15/#3-detecting-the-new-volume","text":"Initially, the new volume /dev/sdb was not visible. To detect it, we ran: sudo udevadm trigger Afterwards, listing block devices confirmed its presence: lsblk Output (before partitioning): sdb 8:16 0 10G 0 disk","title":"3. Detecting the New Volume"},{"location":"ex15/#4-partitioning-the-volume","text":"We used fdisk to create two primary partitions of ~5 GB each: sudo fdisk /dev/sdb Steps: n \u2192 create new partition Type: p \u2192 primary Size: +5G Repeated for second partition Saved with w Resulting devices: /dev/sdb1 5G /dev/sdb2 5G","title":"4. Partitioning the Volume"},{"location":"ex15/#5-creating-file-systems","text":"We formatted the new partitions with different file systems: sudo mkfs -t ext4 /dev/sdb1 sudo mkfs -t xfs /dev/sdb2","title":"5. Creating File Systems"},{"location":"ex15/#6-mounting-the-partitions","text":"We created mount points: sudo mkdir /disk1 /disk2 Mounted them as follows: sudo mount /dev/sdb1 /disk1 sudo mount UUID=$(sudo blkid -s UUID -o value /dev/sdb2) /disk2 Verification: df -h Showed /disk1 and /disk2 mounted successfully.","title":"6. Mounting the Partitions"},{"location":"ex15/#7-persistent-mounts-with-fstab","text":"We edited /etc/fstab to ensure partitions mount automatically after reboot. Entries added: /dev/sdb1 /disk1 ext4 defaults 0 0 UUID=<UUID-of-sdb2> /disk2 xfs defaults 0 0 Then tested with: sudo mount -a No errors occurred, confirming the syntax was valid.","title":"7. Persistent Mounts with fstab"},{"location":"ex15/#8-verification-after-reboot","text":"After rebooting the system, both partitions were automatically mounted. df -h Confirmed that /disk1 (ext4) and /disk2 (xfs) were available.","title":"8. Verification After Reboot"},{"location":"ex15/#9-problems-encountered","text":"Volume not visible at first \u2192 Solved with sudo udevadm trigger . fstab UUID mistake \u2192 Accidentally typed placeholder instead of real UUID. Corrected using sudo blkid . fstab syntax caution \u2192 Verified with mount -a before reboot to avoid boot issues.","title":"9. Problems Encountered"},{"location":"ex16/","text":"Exercise 16: Mount Point\u2019s Name Specification 1. Goal The goal of this exercise was to replace the auto-generated Cloud-init mount point (e.g., /mnt/HC_Volume_102626366 ) with a custom mount point /volume01 . This required creating the server and volume independently, attaching the volume manually ( automount = false ), and configuring the mount via Cloud-init so that it persists after reboot. 2. Terraform Configuration We extended the configuration from Exercise 15. Server and volume created independently Both placed in the same location ( hel1 ) Volume attached manually with automount = false Passed the Linux device path and mount point name to Cloud-init Terraform Snippet resource \"hcloud_volume\" \"volume01\" { name = \"volume1-ex16\" size = 10 location = \"hel1\" format = \"xfs\" } resource \"hcloud_volume_attachment\" \"volAttach\" { volume_id = hcloud_volume.volume01.id server_id = hcloud_server.exercise_14.id automount = false } output \"volume_linux_device\" { description = \"The volume's linux device\" value = hcloud_volume.volume01.linux_device } 3. Cloud-init Configuration We modified the Cloud-init template to: Create the mount point /volume01 Append an entry to /etc/fstab using the - UUID of the volume Reload systemd to account for fstab changes Mount the volume immediately Cloud-init Snippet #cloud-config runcmd: - mkdir -p /volume01 - echo \"UUID=a3de7773-6b5e-4f51-886a-bbccf85d0b32 /volume01 xfs defaults 0 0\" >> /etc/fstab - systemctl daemon-reload - mount -a 4. Deployment and Verification Applied the configuration: terraform apply Verified that the new volume appeared: lsblk Result showed /dev/sdb with 10 GB. Mounted the volume manually for testing: sudo mkdir -p /volume01 sudo mount /dev/disk/by-id/scsi-0HC_Volume_102995935 /volume01 Checked the UUID and updated /etc/fstab: sudo blkid /dev/disk/by-id/scsi-0HC_Volume_102995935 Reloaded systemd and mounted all filesystems: sudo systemctl daemon-reload sudo mount -a Verified with: df -h Output confirmed: /dev/sdb 10G 104M 9.9G 2% /volume01 Tested write access: echo \"Test successful!\" | sudo tee /volume01/test.txt","title":"Exercise 16"},{"location":"ex16/#exercise-16-mount-points-name-specification","text":"","title":"Exercise 16: Mount Point\u2019s Name Specification"},{"location":"ex16/#1-goal","text":"The goal of this exercise was to replace the auto-generated Cloud-init mount point (e.g., /mnt/HC_Volume_102626366 ) with a custom mount point /volume01 . This required creating the server and volume independently, attaching the volume manually ( automount = false ), and configuring the mount via Cloud-init so that it persists after reboot.","title":"1. Goal"},{"location":"ex16/#2-terraform-configuration","text":"We extended the configuration from Exercise 15. Server and volume created independently Both placed in the same location ( hel1 ) Volume attached manually with automount = false Passed the Linux device path and mount point name to Cloud-init","title":"2. Terraform Configuration"},{"location":"ex16/#terraform-snippet","text":"resource \"hcloud_volume\" \"volume01\" { name = \"volume1-ex16\" size = 10 location = \"hel1\" format = \"xfs\" } resource \"hcloud_volume_attachment\" \"volAttach\" { volume_id = hcloud_volume.volume01.id server_id = hcloud_server.exercise_14.id automount = false } output \"volume_linux_device\" { description = \"The volume's linux device\" value = hcloud_volume.volume01.linux_device }","title":"Terraform Snippet"},{"location":"ex16/#3-cloud-init-configuration","text":"We modified the Cloud-init template to: Create the mount point /volume01 Append an entry to /etc/fstab using the - UUID of the volume Reload systemd to account for fstab changes Mount the volume immediately","title":"3. Cloud-init Configuration"},{"location":"ex16/#cloud-init-snippet","text":"#cloud-config runcmd: - mkdir -p /volume01 - echo \"UUID=a3de7773-6b5e-4f51-886a-bbccf85d0b32 /volume01 xfs defaults 0 0\" >> /etc/fstab - systemctl daemon-reload - mount -a","title":"Cloud-init Snippet"},{"location":"ex16/#4-deployment-and-verification","text":"Applied the configuration: terraform apply Verified that the new volume appeared: lsblk Result showed /dev/sdb with 10 GB. Mounted the volume manually for testing: sudo mkdir -p /volume01 sudo mount /dev/disk/by-id/scsi-0HC_Volume_102995935 /volume01 Checked the UUID and updated /etc/fstab: sudo blkid /dev/disk/by-id/scsi-0HC_Volume_102995935 Reloaded systemd and mounted all filesystems: sudo systemctl daemon-reload sudo mount -a Verified with: df -h Output confirmed: /dev/sdb 10G 104M 9.9G 2% /volume01 Tested write access: echo \"Test successful!\" | sudo tee /volume01/test.txt","title":"4. Deployment and Verification"},{"location":"ex17/","text":"Exercise 17: SSH Known Hosts Module Objective The objective of this exercise was to create a reusable Terraform module called SshKnownHosts that automatically generates: a known_hosts file, SSH and SCP wrapper scripts so that connecting to servers can be done securely and consistently without repeating configuration snippets in every project. 2. Project Structure We organized the project into two main directories: . KnownHostsByModule bin ssh scp gen known_hosts main.tf network.tf outputs.tf providers.tf Readme.md tpl userData.yml variables.tf Modules SshKnownHosts main.tf Readme.md tpl ssh.sh scp.sh variables.tf Modules/SshKnownHosts : reusable Terraform module for known_hosts handling KnownHostsByModule : main project using the module 3. Module Definition Variables ( Modules/SshKnownHosts/variables.tf ) variable \"loginUserName\" { description = \"The SSH login user\" type = string } variable \"serverNameOrIp\" { description = \"Server IPv4 address or hostname\" type = string } variable \"serverHostPublicKey\" { description = \"The server's host public key\" type = string } Resources ( Modules/SshKnownHosts/main.tf ) Generate known_hosts resource \"local_file\" \"known_hosts\" { content = \"${var.serverNameOrIp} ${var.serverHostPublicKey}\" filename = \"${path.module}/../KnownHostsByModule/gen/known_hosts\" } Generate ssh wrapper resource \"local_file\" \"ssh_script\" { content = templatefile(\"${path.module}/tpl/ssh.sh\", { loginUser = var.loginUserName server_ip = var.serverNameOrIp }) filename = \"${path.module}/../KnownHostsByModule/bin/ssh\" file_permission = \"700\" } Generate scp wrapper resource \"local_file\" \"scp_script\" { content = templatefile(\"${path.module}/tpl/scp.sh\", { loginUser = var.loginUserName server_ip = var.serverNameOrIp }) filename = \"${path.module}/../KnownHostsByModule/bin/scp\" file_permission = \"700\" } 4. Wrapper Templates tpl/ssh.sh #!/bin/bash GEN_DIR=$(dirname \"$0\")/../gen ssh -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" ${loginUser}@${server_ip} \"$@\" tpl/scp.sh #!/bin/bash GEN_DIR=$(dirname \"$0\")/../gen if [ $# -lt 2 ]; then echo \"Usage: ./bin/scp <source> <destination>\" else scp -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" \"$@\" fi 5. Module Usage in Main Project module \"createSshKnownHosts\" { source = \"../Modules/SshKnownHosts\" loginUserName = hcloud_ssh_key.loginUser.name serverNameOrIp = hcloud_server.exercise_14.ipv4_address serverHostPublicKey = tls_private_key.host.public_key_openssh } 6. Verification Applied Terraform configuration: terraform apply Confirmed generation of: gen/known_hosts Executable scripts in bin/ssh and bin/scp Set permissions: chmod +x bin/ssh bin/scp Tested SSH login: ./bin/ssh \u2192 Logged in successfully without host key warning. Tested SCP file transfer: ./bin/scp localfile.txt devops@<server_ip>:/home/devops/ \u2192 File appeared on server.","title":"Exercise 17"},{"location":"ex17/#exercise-17-ssh-known-hosts-module","text":"Objective The objective of this exercise was to create a reusable Terraform module called SshKnownHosts that automatically generates: a known_hosts file, SSH and SCP wrapper scripts so that connecting to servers can be done securely and consistently without repeating configuration snippets in every project.","title":"Exercise 17: SSH Known Hosts Module"},{"location":"ex17/#2-project-structure","text":"We organized the project into two main directories: . KnownHostsByModule bin ssh scp gen known_hosts main.tf network.tf outputs.tf providers.tf Readme.md tpl userData.yml variables.tf Modules SshKnownHosts main.tf Readme.md tpl ssh.sh scp.sh variables.tf Modules/SshKnownHosts : reusable Terraform module for known_hosts handling KnownHostsByModule : main project using the module","title":"2. Project Structure"},{"location":"ex17/#3-module-definition","text":"Variables ( Modules/SshKnownHosts/variables.tf ) variable \"loginUserName\" { description = \"The SSH login user\" type = string } variable \"serverNameOrIp\" { description = \"Server IPv4 address or hostname\" type = string } variable \"serverHostPublicKey\" { description = \"The server's host public key\" type = string }","title":"3. Module Definition"},{"location":"ex17/#resources-modulessshknownhostsmaintf","text":"Generate known_hosts resource \"local_file\" \"known_hosts\" { content = \"${var.serverNameOrIp} ${var.serverHostPublicKey}\" filename = \"${path.module}/../KnownHostsByModule/gen/known_hosts\" } Generate ssh wrapper resource \"local_file\" \"ssh_script\" { content = templatefile(\"${path.module}/tpl/ssh.sh\", { loginUser = var.loginUserName server_ip = var.serverNameOrIp }) filename = \"${path.module}/../KnownHostsByModule/bin/ssh\" file_permission = \"700\" } Generate scp wrapper resource \"local_file\" \"scp_script\" { content = templatefile(\"${path.module}/tpl/scp.sh\", { loginUser = var.loginUserName server_ip = var.serverNameOrIp }) filename = \"${path.module}/../KnownHostsByModule/bin/scp\" file_permission = \"700\" }","title":"Resources (Modules/SshKnownHosts/main.tf)"},{"location":"ex17/#4-wrapper-templates","text":"tpl/ssh.sh #!/bin/bash GEN_DIR=$(dirname \"$0\")/../gen ssh -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" ${loginUser}@${server_ip} \"$@\" tpl/scp.sh #!/bin/bash GEN_DIR=$(dirname \"$0\")/../gen if [ $# -lt 2 ]; then echo \"Usage: ./bin/scp <source> <destination>\" else scp -o UserKnownHostsFile=\"$GEN_DIR/known_hosts\" \"$@\" fi","title":"4. Wrapper Templates"},{"location":"ex17/#5-module-usage-in-main-project","text":"module \"createSshKnownHosts\" { source = \"../Modules/SshKnownHosts\" loginUserName = hcloud_ssh_key.loginUser.name serverNameOrIp = hcloud_server.exercise_14.ipv4_address serverHostPublicKey = tls_private_key.host.public_key_openssh }","title":"5. Module Usage in Main Project"},{"location":"ex17/#6-verification","text":"Applied Terraform configuration: terraform apply Confirmed generation of: gen/known_hosts Executable scripts in bin/ssh and bin/scp Set permissions: chmod +x bin/ssh bin/scp Tested SSH login: ./bin/ssh \u2192 Logged in successfully without host key warning. Tested SCP file transfer: ./bin/scp localfile.txt devops@<server_ip>:/home/devops/ \u2192 File appeared on server.","title":"6. Verification"},{"location":"ex18/","text":"Exercise 18: Enhancing Your Web Server Objective The goal of this exercise was to enhance our existing web server setup by: Adding DNS A records to resolve a human-readable domain to our server\u2019s IP. Configuring Nginx to serve the site under the new domain. Securing the site with HTTPS using Let\u2019s Encrypt TLS certificates. Steps: 1. DNS A Records Setup We created two DNS entries pointing to our server\u2019s IP ( 95.216.223.223 ) using nsupdate with the provided key: nsupdate -k ./g1.key > server ns1.hdm-stuttgart.cloud > update add www.g1.sdi.hdm-stuttgart.cloud 10 A 95.216.223.223 > update add g1.sdi.hdm-stuttgart.cloud 10 A 95.216.223.223 > send > quit Verification with dig confirmed the DNS resolution worked: dig +noall +answer @ns1.hdm-stuttgart.cloud g1.sdi.hdm-stuttgart.cloud Result: g1.sdi.hdm-stuttgart.cloud. 10 IN A 95.216.223.223 Both g1.sdi.hdm-stuttgart.cloud and www.g1.sdi.hdm-stuttgart.cloud resolved to the server IP. 2. Configuring Nginx for the Domain We set up a new Nginx server block to handle requests for both domain names: /etc/nginx/sites-available/g1.sdi.hdm-stuttgart.cloud server { listen 80; listen [::]:80; root /var/www/g1.sdi.hdm-stuttgart.cloud/html; index index.html index.htm; server_name g1.sdi.hdm-stuttgart.cloud www.g1.sdi.hdm-stuttgart.cloud; location / { try_files $uri $uri/ =404; } } Then we tested the configuration and restarted Nginx: sudo nginx -t sudo systemctl restart nginx Opening the domain in the browser showed the Nginx landing page with our custom message. 3. Securing with Let\u2019s Encrypt TLS Certificates We installed and used Certbot to request certificates for both domain variants. First, we used a staging certificate to avoid Let\u2019s Encrypt rate limits: sudo certbot --nginx --test-cert -d g1.sdi.hdm-stuttgart.cloud -d www.g1.sdi.hdm-stuttgart.cloud After verifying success, we requested the real certificate: sudo certbot --nginx -d g1.sdi.hdm-stuttgart.cloud -d www.g1.sdi.hdm-stuttgart.cloud Finally, we confirmed automatic renewal works: sudo certbot renew --dry-run Output confirmed that the certificates would renew successfully. Final Verification Access to both http://g1.sdi.hdm-stuttgart.cloud and http://www.g1.sdi.hdm-stuttgart.cloud worked. HTTPS access confirmed with a valid Let\u2019s Encrypt certificate: https://g1.sdi.hdm-stuttgart.cloud https://www.g1.sdi.hdm-stuttgart.cloud Automatic certificate renewal tested successfully. Screenshot evidence: DNS update and verification with dig . Browser access via domain name. Certbot successful certificate deployment. Dry-run renewal showing success.","title":"Exercise 18"},{"location":"ex18/#exercise-18-enhancing-your-web-server","text":"","title":"Exercise 18: Enhancing Your Web Server"},{"location":"ex18/#objective","text":"The goal of this exercise was to enhance our existing web server setup by: Adding DNS A records to resolve a human-readable domain to our server\u2019s IP. Configuring Nginx to serve the site under the new domain. Securing the site with HTTPS using Let\u2019s Encrypt TLS certificates.","title":"Objective"},{"location":"ex18/#steps","text":"","title":"Steps:"},{"location":"ex18/#1-dns-a-records-setup","text":"We created two DNS entries pointing to our server\u2019s IP ( 95.216.223.223 ) using nsupdate with the provided key: nsupdate -k ./g1.key > server ns1.hdm-stuttgart.cloud > update add www.g1.sdi.hdm-stuttgart.cloud 10 A 95.216.223.223 > update add g1.sdi.hdm-stuttgart.cloud 10 A 95.216.223.223 > send > quit Verification with dig confirmed the DNS resolution worked: dig +noall +answer @ns1.hdm-stuttgart.cloud g1.sdi.hdm-stuttgart.cloud Result: g1.sdi.hdm-stuttgart.cloud. 10 IN A 95.216.223.223 Both g1.sdi.hdm-stuttgart.cloud and www.g1.sdi.hdm-stuttgart.cloud resolved to the server IP.","title":"1. DNS A Records Setup"},{"location":"ex18/#2-configuring-nginx-for-the-domain","text":"We set up a new Nginx server block to handle requests for both domain names: /etc/nginx/sites-available/g1.sdi.hdm-stuttgart.cloud server { listen 80; listen [::]:80; root /var/www/g1.sdi.hdm-stuttgart.cloud/html; index index.html index.htm; server_name g1.sdi.hdm-stuttgart.cloud www.g1.sdi.hdm-stuttgart.cloud; location / { try_files $uri $uri/ =404; } } Then we tested the configuration and restarted Nginx: sudo nginx -t sudo systemctl restart nginx Opening the domain in the browser showed the Nginx landing page with our custom message.","title":"2. Configuring Nginx for the Domain"},{"location":"ex18/#3-securing-with-lets-encrypt-tls-certificates","text":"We installed and used Certbot to request certificates for both domain variants. First, we used a staging certificate to avoid Let\u2019s Encrypt rate limits: sudo certbot --nginx --test-cert -d g1.sdi.hdm-stuttgart.cloud -d www.g1.sdi.hdm-stuttgart.cloud After verifying success, we requested the real certificate: sudo certbot --nginx -d g1.sdi.hdm-stuttgart.cloud -d www.g1.sdi.hdm-stuttgart.cloud Finally, we confirmed automatic renewal works: sudo certbot renew --dry-run Output confirmed that the certificates would renew successfully.","title":"3. Securing with Let\u2019s Encrypt TLS Certificates"},{"location":"ex18/#final-verification","text":"Access to both http://g1.sdi.hdm-stuttgart.cloud and http://www.g1.sdi.hdm-stuttgart.cloud worked. HTTPS access confirmed with a valid Let\u2019s Encrypt certificate: https://g1.sdi.hdm-stuttgart.cloud https://www.g1.sdi.hdm-stuttgart.cloud Automatic certificate renewal tested successfully.","title":"Final Verification"},{"location":"ex18/#screenshot-evidence","text":"DNS update and verification with dig . Browser access via domain name. Certbot successful certificate deployment. Dry-run renewal showing success.","title":"Screenshot evidence:"},{"location":"ex19/","text":"Exercise 19: Creating DNS Records with Terraform Implementation 1. Variables Defined in variables.tf and config.auto.tfvars : serverIp = \"1.2.3.4\" dnsZone = \"g1.sdi.hdm-stuttgart.cloud\" serverName = \"workhorse\" serverAliases = [\"www\", \"mail\"] Validation rules ensured: No duplicate aliases ( distinct function). No alias equal to the canonical name ( contains function). 2. Providers Hetzner Cloud Provider \u2192 for server and firewall. DNS Provider (hashicorp/dns) \u2192 with HMAC-SHA512 authentication against ns1.hdm-stuttgart.cloud . 3. Terraform Resources A Record for Base Domain: resource \"dns_a_record_set\" \"base_record\" { zone = \"${var.dnsZone}.\" addresses = [var.serverIp] ttl = 10 } A Record for Canonical Server Name: resource \"dns_a_record_set\" \"server_record\" { zone = \"${var.dnsZone}.\" name = var.serverName addresses = [var.serverIp] ttl = 10 } CNAME Records for Aliases: resource \"dns_cname_record\" \"alias_records\" { for_each = toset(var.serverAliases) zone = \"${var.dnsZone}.\" name = each.key cname = \"${var.serverName}.${var.dnsZone}.\" ttl = 10 } Verification Executed: dig +noall +answer \\ @ns1.hdm-stuttgart.cloud \\ -y hmac-sha512:g1.key:<SECRET> \\ -t AXFR g1.sdi.hdm-stuttgart.cloud Output: g1.sdi.hdm-stuttgart.cloud. 10 IN A 157.180.35.105 workhorse.g1.sdi.hdm-stuttgart.cloud. 10 IN A 157.180.35.105 mail.g1.sdi.hdm-stuttgart.cloud. 10 IN CNAME workhorse.g1.sdi.hdm-stuttgart.cloud. www.g1.sdi.hdm-stuttgart.cloud. 10 IN CNAME workhorse.g1.sdi.hdm-stuttgart.cloud.","title":"Exercise 19"},{"location":"ex19/#exercise-19-creating-dns-records-with-terraform","text":"","title":"Exercise 19: Creating DNS Records with Terraform"},{"location":"ex19/#implementation","text":"","title":"Implementation"},{"location":"ex19/#1-variables","text":"Defined in variables.tf and config.auto.tfvars : serverIp = \"1.2.3.4\" dnsZone = \"g1.sdi.hdm-stuttgart.cloud\" serverName = \"workhorse\" serverAliases = [\"www\", \"mail\"] Validation rules ensured: No duplicate aliases ( distinct function). No alias equal to the canonical name ( contains function).","title":"1. Variables"},{"location":"ex19/#2-providers","text":"Hetzner Cloud Provider \u2192 for server and firewall. DNS Provider (hashicorp/dns) \u2192 with HMAC-SHA512 authentication against ns1.hdm-stuttgart.cloud .","title":"2. Providers"},{"location":"ex19/#3-terraform-resources","text":"A Record for Base Domain: resource \"dns_a_record_set\" \"base_record\" { zone = \"${var.dnsZone}.\" addresses = [var.serverIp] ttl = 10 } A Record for Canonical Server Name: resource \"dns_a_record_set\" \"server_record\" { zone = \"${var.dnsZone}.\" name = var.serverName addresses = [var.serverIp] ttl = 10 } CNAME Records for Aliases: resource \"dns_cname_record\" \"alias_records\" { for_each = toset(var.serverAliases) zone = \"${var.dnsZone}.\" name = each.key cname = \"${var.serverName}.${var.dnsZone}.\" ttl = 10 }","title":"3. Terraform Resources"},{"location":"ex19/#verification","text":"Executed: dig +noall +answer \\ @ns1.hdm-stuttgart.cloud \\ -y hmac-sha512:g1.key:<SECRET> \\ -t AXFR g1.sdi.hdm-stuttgart.cloud Output: g1.sdi.hdm-stuttgart.cloud. 10 IN A 157.180.35.105 workhorse.g1.sdi.hdm-stuttgart.cloud. 10 IN A 157.180.35.105 mail.g1.sdi.hdm-stuttgart.cloud. 10 IN CNAME workhorse.g1.sdi.hdm-stuttgart.cloud. www.g1.sdi.hdm-stuttgart.cloud. 10 IN CNAME workhorse.g1.sdi.hdm-stuttgart.cloud.","title":"Verification"},{"location":"ex2/","text":"","title":"Exercise 2"},{"location":"ex20/","text":"Exercise 20: Creating a Host with Corresponding DNS Entries Implementation 1. Extending Terraform with the SSH Known Hosts Module In main.tf , the createSshKnownHosts module was used with the DNS name instead of the raw IP: module \"createSshKnownHosts\" { source = \"../Modules/SshKnownHosts\" loginUserName = \"devops\" serverNameOrIp = \"${var.serverName}.${var.dnsZone}\" serverHostPublicKey = tls_private_key.host.public_key_openssh } serverNameOrIp now points to workhorse.g1.sdi.hdm-stuttgart.cloud . This ensures generated SSH and SCP wrapper scripts, as well as the known_hosts file, rely on the DNS hostname. 2. Planning and Applying Terraform Ran the following commands: terraform plan -var-file=\"secrets.tfvars\" terraform apply -var-file=\"secrets.tfvars\" Result: Plan: 3 to add, 0 to change, 0 to destroy. Apply complete! Resources: 3 added, 0 changed, 0 destroyed. Generated resources: gen/known_hosts bin/ssh bin/scp 3. Generated Files gen/known_hosts workhorse.g1.sdi.hdm-stuttgart.cloud ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBGurzVlZ0fp9cI6WYpuyFsrFTlpWWLJp4EU3iXj4Voy bin/ssh #!/bin/bash ssh -o UserKnownHostsFile=$(dirname \"$0\")/../gen/known_hosts \\ devops@workhorse.g1.sdi.hdm-stuttgart.cloud \"$@\" bin/scp #!/bin/bash scp -o UserKnownHostsFile=$(dirname \"$0\")/../gen/known_hosts \\ \"$@\" devops@workhorse.g1.sdi.hdm-stuttgart.cloud: 4. Verification Executed: ./bin/ssh Result: Successful login to the Debian host via DNS name. No more IP mismatch warnings. SSH and SCP commands now resolve cleanly using the DNS hostname.","title":"Exercise 20"},{"location":"ex20/#exercise-20-creating-a-host-with-corresponding-dns-entries","text":"","title":"Exercise 20: Creating a Host with Corresponding DNS Entries"},{"location":"ex20/#implementation","text":"","title":"Implementation"},{"location":"ex20/#1-extending-terraform-with-the-ssh-known-hosts-module","text":"In main.tf , the createSshKnownHosts module was used with the DNS name instead of the raw IP: module \"createSshKnownHosts\" { source = \"../Modules/SshKnownHosts\" loginUserName = \"devops\" serverNameOrIp = \"${var.serverName}.${var.dnsZone}\" serverHostPublicKey = tls_private_key.host.public_key_openssh } serverNameOrIp now points to workhorse.g1.sdi.hdm-stuttgart.cloud . This ensures generated SSH and SCP wrapper scripts, as well as the known_hosts file, rely on the DNS hostname.","title":"1. Extending Terraform with the SSH Known Hosts Module"},{"location":"ex20/#2-planning-and-applying-terraform","text":"Ran the following commands: terraform plan -var-file=\"secrets.tfvars\" terraform apply -var-file=\"secrets.tfvars\" Result: Plan: 3 to add, 0 to change, 0 to destroy. Apply complete! Resources: 3 added, 0 changed, 0 destroyed. Generated resources: gen/known_hosts bin/ssh bin/scp","title":"2. Planning and Applying Terraform"},{"location":"ex20/#3-generated-files","text":"gen/known_hosts workhorse.g1.sdi.hdm-stuttgart.cloud ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBGurzVlZ0fp9cI6WYpuyFsrFTlpWWLJp4EU3iXj4Voy bin/ssh #!/bin/bash ssh -o UserKnownHostsFile=$(dirname \"$0\")/../gen/known_hosts \\ devops@workhorse.g1.sdi.hdm-stuttgart.cloud \"$@\" bin/scp #!/bin/bash scp -o UserKnownHostsFile=$(dirname \"$0\")/../gen/known_hosts \\ \"$@\" devops@workhorse.g1.sdi.hdm-stuttgart.cloud:","title":"3. Generated Files"},{"location":"ex20/#4-verification","text":"Executed: ./bin/ssh Result: Successful login to the Debian host via DNS name. No more IP mismatch warnings. SSH and SCP commands now resolve cleanly using the DNS hostname.","title":"4. Verification"},{"location":"ex3/","text":"","title":"Exercise 3"},{"location":"ex4/","text":"","title":"Exercise 4"},{"location":"ex5/","text":"","title":"Exercise 5"},{"location":"ex6/","text":"","title":"Exercise 6"},{"location":"ex7/","text":"","title":"Exercise 7"},{"location":"ex8/","text":"","title":"Exercise 8"},{"location":"ex9/","text":"","title":"Exercise 9"}]}